{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PSPNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TIMEdilation584/JP_Loksatta_moving_hearts/blob/master/June_working_PSPNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05KTXP7XoABy"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Stpqi6N6oC4Q",
        "outputId": "931cf271-84c0-454f-8655-3b1a856b6f87"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztrS-_z9nr42",
        "outputId": "df9c8120-7c4a-4228-872c-4f802637680d"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from imutils import paths\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from glob import glob\n",
        "import IPython.display as display\n",
        "from IPython.display import clear_output\n",
        "import math\n",
        "import time\n",
        "from tensorflow.keras.layers import *\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# For more information about autotune:\n",
        "# https://www.tensorflow.org/guide/data_performance#prefetching\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "print(f'Tensorflow ver. {tf.__version__}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow ver. 2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlQWmSTdnr48"
      },
      "source": [
        "## Loading the Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsHopd_8nr49"
      },
      "source": [
        "# important for reproducibility\n",
        "# this allows to generate the same random numbers\n",
        "SEED = 42\n",
        "\n",
        "# Dataset path\n",
        "dataset_path = 'drive/My Drive/idd-lite/idd20k_lite/'\n",
        "img_train = dataset_path + 'leftImg8bit/train/'\n",
        "seg_train = dataset_path + 'gtFine/train/'\n",
        "\n",
        "img_val = dataset_path + 'leftImg8bit/val/'\n",
        "seg_val = dataset_path + 'gtFine/val/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJXKKV05nr5i"
      },
      "source": [
        "## Data Preparation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bp5XLcYnr5k"
      },
      "source": [
        "# Image size that we are going to use\n",
        "(HEIGHT,WIDTH) = (128,128)\n",
        "# Our images are RGB (3 channels)\n",
        "N_CHANNELS = 3\n",
        "# Scene Parsing has 7 classes (0-6) + `not labeled`\n",
        "N_CLASSES = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8_AtJ8Bnr5m",
        "outputId": "e1b3f5f1-4e95-4e68-ae2f-1a98cc722293"
      },
      "source": [
        "# Reference -> https://docs.python.org/2/library/glob.html\n",
        "# The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, \n",
        "# although results are returned in arbitrary order. \n",
        "\n",
        "TRAINSET_SIZE = len(glob(img_train+'*/*_image.jpg'))\n",
        "print(f\"The Training Dataset contains {TRAINSET_SIZE} images.\")\n",
        "\n",
        "VALSET_SIZE = len(glob(img_val+'*/*_image.jpg'))\n",
        "print(f\"The Validation Dataset contains {VALSET_SIZE} images.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Training Dataset contains 1403 images.\n",
            "The Validation Dataset contains 204 images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ksntANjnr5p"
      },
      "source": [
        "def parse_image(img_path):\n",
        "    \"\"\"\n",
        "    Load an image and its annotation (mask) and returning a dictionary.\n",
        "    \"\"\"\n",
        "    # Reading the image\n",
        "    image = tf.io.read_file(img_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # For one Image path:\n",
        "    # .../idd20k_lite/leftImg8bit/train/024541_image.jpg\n",
        "    # Its corresponding annotation path is:\n",
        "    # .../idd20k_lite/gtFine/train/024541_label.png\n",
        "    mask_path = tf.strings.regex_replace(img_path, \"leftImg8bit\", \"gtFine\")\n",
        "    mask_path = tf.strings.regex_replace(mask_path, \"_image.jpg\", \"_label.png\")\n",
        "    # Reading the annotation file corresponding the image file\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    # In scene parsing, \"not labeled\" = 255\n",
        "    # But it will mess with our N_CLASS = 7\n",
        "    # Since 255 means the 255th class\n",
        "    # Which doesn't exist\n",
        "    mask = tf.where(mask==255, np.dtype('uint8').type(7), mask)\n",
        "    # Note that we have to convert the new value (7)\n",
        "    # With the same dtype than the tensor itself\n",
        "    return {'image': image, 'segmentation_mask': mask}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIxF4YUanr5s"
      },
      "source": [
        "# Reference -> https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files\n",
        "# tf.data.Dataset.list_files returns a dataset of all files matching one or more glob patterns.\n",
        "\n",
        "train_dataset = tf.data.Dataset.list_files(img_train+'*/*_image.jpg', seed=SEED)\n",
        "train_dataset = train_dataset.map(parse_image)\n",
        "\n",
        "val_dataset = tf.data.Dataset.list_files(img_val+'*/*_image.jpg', seed=SEED)\n",
        "val_dataset =val_dataset.map(parse_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRpudDSOnr5v"
      },
      "source": [
        "# Reference -> https://www.tensorflow.org/api_docs/python/tf/cast\n",
        "# Returns a Tensor same shape as given tensor and same type as dtype that is mentioned.\n",
        "\n",
        "def normalize(input_image, input_mask):\n",
        "    \"\"\"\n",
        "    Rescale the pixel values of the images between 0 and 1 compared to [0,255] originally.\n",
        "    \"\"\"\n",
        "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "    return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ1AAuYlnr5x"
      },
      "source": [
        "# Reference -> https://www.tensorflow.org/api_docs/python/tf/image/resize\n",
        "# Resize images to the size specified\n",
        "\n",
        "def load_image_train(datapoint):\n",
        "    \"\"\"\n",
        "    Normalize and resize a train image and its annotation.\n",
        "    Apply random transformations to an input dictionary containing a train image and its annotation.\n",
        "    \"\"\"\n",
        "    input_image = tf.image.resize(datapoint['image'], (HEIGHT,WIDTH))\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT,WIDTH))\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        input_image = tf.image.flip_left_right(input_image)\n",
        "        input_mask = tf.image.flip_left_right(input_mask)\n",
        "    input_image, input_mask = normalize(input_image, input_mask)\n",
        "    return input_image, input_mask\n",
        "\n",
        "def load_image_test(datapoint):\n",
        "    \"\"\"\n",
        "    Normalize and resize a test image and its annotation.\n",
        "    Since this is for the test set, we don't need to apply any data augmentation technique.\n",
        "    \"\"\"\n",
        "    input_image = tf.image.resize(datapoint['image'], (HEIGHT,WIDTH))\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT,WIDTH))\n",
        "    input_image, input_mask = normalize(input_image, input_mask)\n",
        "    return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQF444Qcnr5z"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = 1500\n",
        "dataset = {\"train\": train_dataset, \"val\": val_dataset}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3n8IUocnr52",
        "outputId": "b6c7dfe7-5c9d-4024-c0c7-13059af69200"
      },
      "source": [
        "# Reference -> https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
        "\n",
        "# Preparing the Train dataset by applying dataset transformations\n",
        "dataset['train'] = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
        "dataset['train'] = dataset['train'].repeat()\n",
        "dataset['train'] = dataset['train'].batch(BATCH_SIZE)\n",
        "dataset['train'] = dataset['train'].prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset['train'])\n",
        "\n",
        "# Preparing the Validation Dataset\n",
        "dataset['val'] = dataset['val'].map(load_image_test)\n",
        "dataset['val'] = dataset['val'].repeat()\n",
        "dataset['val'] = dataset['val'].batch(BATCH_SIZE)\n",
        "dataset['val'] = dataset['val'].prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset['val'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((None, 128, 128, 3), (None, 128, 128, 1)), types: (tf.float32, tf.float32)>\n",
            "<PrefetchDataset shapes: ((None, 128, 128, 3), (None, 128, 128, 1)), types: (tf.float32, tf.float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPzSSDp4_4Hx"
      },
      "source": [
        "# PSPNet from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDEAJfhTIPuB"
      },
      "source": [
        "# A class to get the basic convolution block \n",
        "# convolution_layer -> batch_normalization -> activation\n",
        "class BasicConvolutionBlock(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, filter_size, kernel_size, dilation_rate, alpha, block_number, initializer='he_normal'):\n",
        "        super(BasicConvolutionBlock, self).__init__()\n",
        "        self.block_number = block_number\n",
        "        self.convolution_layer = Convolution2D(filter_size, kernel_size =  kernel_size, \n",
        "                                               dilation_rate = dilation_rate, padding = 'same',\n",
        "                                               kernel_initializer = initializer)\n",
        "        self.batch_normalization = BatchNormalization()\n",
        "        self.activation = LeakyReLU(alpha = alpha)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        conv_output = self.convolution_layer(inputs)\n",
        "        norm_output = self.batch_normalization(conv_output)\n",
        "        if(self.block_number >= 3):\n",
        "            return norm_output\n",
        "        activation_output = self.activation(norm_output)  \n",
        "        return activation_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua2CSUuvIPuD"
      },
      "source": [
        "# A Building Block of Residual Network\n",
        "class ConvolutionBlock(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, filters, initializer='he_normal'):     \n",
        "        super(ConvolutionBlock, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.basic_convolution_1 = BasicConvolutionBlock(self.filters[0], (1,1), (1,1), 0.2, 1)\n",
        "        self.basic_convolution_2 = BasicConvolutionBlock(self.filters[1], (3,3), (2,2), 0.2, 2)\n",
        "        self.basic_convolution_3 = BasicConvolutionBlock(self.filters[2], (1,1), (1,1), None, 3)\n",
        "        self.skip_convolution = BasicConvolutionBlock(self.filters[2], (3,3), (1,1), None, 4)\n",
        "        # Last Block\n",
        "        self.add_layer = Add()\n",
        "        self.relu_activation = ReLU()\n",
        "     \n",
        "    def call(self, inputs):\n",
        "        skip_input = inputs\n",
        "        output_conv_1 = self.basic_convolution_1(inputs)\n",
        "        output_conv_2 = self.basic_convolution_2(output_conv_1)\n",
        "        output_conv_3 = self.basic_convolution_3(output_conv_2)\n",
        "        output_skip_conv = self.skip_convolution(skip_input)\n",
        "        # Last Block\n",
        "        add_output = self.add_layer([output_conv_3,output_skip_conv])\n",
        "        output = self.relu_activation(add_output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rtNpM3_IPuE"
      },
      "source": [
        "# Encoder block for PSPNet Net\n",
        "class EncoderBlock(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, filters):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.convolution_block_1 = ConvolutionBlock(self.filters[0])\n",
        "        self.convolution_block_2 = ConvolutionBlock(self.filters[1])\n",
        "        self.convolution_block_3 = ConvolutionBlock(self.filters[2])\n",
        "        self.convolution_block_4 = ConvolutionBlock(self.filters[3])\n",
        "        \n",
        "    def call(self, inputs):     \n",
        "        output_block_1 = self.convolution_block_1(inputs)\n",
        "        output_block_2 = self.convolution_block_2(output_block_1)\n",
        "        output_block_3 = self.convolution_block_3(output_block_2) \n",
        "        output_block_4 = self.convolution_block_4(output_block_3)\n",
        "        return output_block_4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtEBVUAFIPuG"
      },
      "source": [
        "# This class returns the pyramid feature map for Pyramid Pooling module\n",
        "# Pooling -> Convolution -> UpSampling\n",
        "class PyramidFeatureMap(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, block, filter_size, unsampling_size, pool_size, interpolation='bilinear'):\n",
        "        super(PyramidFeatureMap, self).__init__()\n",
        "        self.block = block\n",
        "        self.convolution_layer = Convolution2D(filter_size, kernel_size = (1,1))\n",
        "        self.upsampling_layer = UpSampling2D(unsampling_size, interpolation = interpolation)\n",
        "        self.average_pooling = AveragePooling2D(pool_size)\n",
        "        self.global_average_pooling = GlobalAveragePooling2D()\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        if(self.block=='red'):\n",
        "            pool_output = self.global_average_pooling(inputs)\n",
        "            pool_output = Reshape((1,1,512))(pool_output)\n",
        "        else:\n",
        "            pool_output = self.average_pooling(inputs)\n",
        "        conv_output = self.convolution_layer(pool_output)\n",
        "        upsampling_output = self.upsampling_layer(conv_output)\n",
        "        return upsampling_output        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp5p-eeQIPuJ"
      },
      "source": [
        "# This class builds the Pyramid Pooling Module\n",
        "# It builds the 4 feature maps and then concatenates all of them with the input\n",
        "class PyramidPoolingModule(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(PyramidPoolingModule, self).__init__()\n",
        "        self.block_red = PyramidFeatureMap('red', 64, (128,128), (1,1))\n",
        "        self.block_yellow = PyramidFeatureMap('yellow', 64, 2, (2,2))\n",
        "        self.block_blue = PyramidFeatureMap('blue', 64, 4, (4,4))\n",
        "        self.block_green = PyramidFeatureMap('green', 64, 8, (8,8))\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        red_output = self.block_red(inputs)\n",
        "        yellow_output = self.block_yellow(inputs)\n",
        "        blue_output = self.block_blue(inputs)\n",
        "        green_output = self.block_green(inputs)\n",
        "        merged = concatenate([inputs, red_output, yellow_output, blue_output, green_output])\n",
        "        return merged  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKlKJ5SVIPuL"
      },
      "source": [
        "# This block is for the last stage\n",
        "# It gets the output of Pyramid Pooling Module and then performs convolution operation\n",
        "# Pyramid Pooling -> Convolution -> Batch Normalization -> Softmax activation\n",
        "class DecoderBlock(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, num_classes):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.pyramid_pooling_module = PyramidPoolingModule()\n",
        "        self.convolution_layer = Convolution2D(num_classes, kernel_size = (3,3), padding = 'same')\n",
        "        self.batch_norm_layer = BatchNormalization()\n",
        "        self.activation = Activation('softmax')\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        merged = self.pyramid_pooling_module(inputs)\n",
        "        conv_output = self.convolution_layer(merged)\n",
        "        norm_output = self.batch_norm_layer(conv_output)\n",
        "        output = self.activation(norm_output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SKIp8d7IPuN"
      },
      "source": [
        "#This final class for the Model\n",
        "class PSPNetModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, num_classes):\n",
        "        super(PSPNetModel, self).__init__()\n",
        "        self.encoder = EncoderBlock([[32,32,64],[64,64,128],[128,128,256],[256,256,512]])\n",
        "        self.decoder = DecoderBlock(num_classes)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        encoder_output = self.encoder(inputs)\n",
        "        final_output = self.decoder(encoder_output)\n",
        "        return final_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf8W7lubEkGk"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CTzjSCt9Fgg"
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "# SegmentationModel object\n",
        "model = PSPNetModel(N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byhYZrak9pOf"
      },
      "source": [
        "# Defining a loss object and an optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, 'tf_ckpts/', max_to_keep=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EpC2stF9nIg"
      },
      "source": [
        "# Define the metrics\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpLXhPka9jc0"
      },
      "source": [
        "# Reference -> https://github.com/junhoning/machine_learning_tutorial/blob/b20b8a10438ec3e62f08f920744cc8ea854cde91/Visualization%20%26%20TensorBoard/%5BTensorBoard%5D%20Semantic%20Segmentation.ipynb\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, optimizer, x_train, y_train):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_train, training=True)\n",
        "        loss = loss_object(y_train, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(y_train, predictions)\n",
        "    \n",
        "def train_and_checkpoint(model, manager, dataset, epoch):\n",
        "    ckpt.restore(manager.latest_checkpoint)\n",
        "    if manager.latest_checkpoint:\n",
        "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "    else:\n",
        "        print(\"Initializing from scratch.\")\n",
        "    for (x_train, y_train) in dataset['train'].take(math.ceil(1403/8)):\n",
        "        train_step(model, optimizer, x_train, y_train)\n",
        "    ckpt.step.assign_add(1)\n",
        "    save_path = manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}: {}\".format(epoch, save_path))\n",
        "    \n",
        "@tf.function\n",
        "def test_step(model, x_test, y_test):\n",
        "    predictions = model(x_test)\n",
        "    loss = loss_object(y_test, predictions)\n",
        "    test_loss(loss)\n",
        "    test_accuracy(y_test, predictions)\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMnqVwF29gZz"
      },
      "source": [
        "train_log_dir = 'logs/gradient_tape/train'\n",
        "test_log_dir = 'logs/gradient_tape/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp8uAicC9ct-",
        "scrolled": true,
        "outputId": "4667071b-f1c7-4583-87ab-0a7932bb9495"
      },
      "source": [
        "# This variable will help to save the best model if its performance increases after an epoch   \n",
        "highest_accuracy = 0\n",
        "\n",
        "# Training the model for 40 epochs\n",
        "for epoch in range(50):\n",
        "\n",
        "    print(\"Epoch \",epoch+1)\n",
        "    \n",
        "    # Getting the current time before starting the training\n",
        "    # This will help to keep track of how much time an epoch took\n",
        "    start = time.time()\n",
        "    \n",
        "    train_and_checkpoint(model, manager, dataset, epoch+1)\n",
        "    \n",
        "    # Saving the train loss and train accuracy metric for TensorBoard visualization\n",
        "    with train_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', train_loss.result(), step=ckpt.step.numpy())\n",
        "        tf.summary.scalar('accuracy', train_accuracy.result(), step=ckpt.step.numpy())\n",
        "    \n",
        "    # Validation phase\n",
        "    for (x_test, y_test) in dataset['val'].take(math.ceil(204/8)):\n",
        "        pred = test_step(model, x_test, y_test)\n",
        "    \n",
        "    # Saving the validation loss and validation accuracy metric for Tensorboard visualization\n",
        "    with test_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', test_loss.result(), step=ckpt.step.numpy())\n",
        "        tf.summary.scalar('accuracy', test_accuracy.result(), step=ckpt.step.numpy())\n",
        "    \n",
        "    # Calculating the time it took for the entire epoch to run\n",
        "    print(\"Time taken \",time.time()-start)\n",
        "    \n",
        "    # Printing the metrics for the epoch\n",
        "    template = 'Epoch {}, Loss: {:.3f}, Accuracy: {:.3f}, Val Loss: {:.3f}, Val Accuracy: {:.3f}'\n",
        "    print (template.format(epoch+1,\n",
        "                            train_loss.result(), \n",
        "                            train_accuracy.result()*100,\n",
        "                            test_loss.result(), \n",
        "                            test_accuracy.result()*100))\n",
        "    \n",
        "    # If accuracy has increased in this epoch, updating the highest accuracy and saving the model\n",
        "    if(test_accuracy.result().numpy()*100>highest_accuracy):\n",
        "        print(\"Validation accuracy increased from {:.3f} to {:.3f}. Saving model weights.\".format(highest_accuracy,test_accuracy.result().numpy()*100))\n",
        "        highest_accuracy = test_accuracy.result().numpy()*100\n",
        "        model.save_weights('pspnet_weights-epoch-{}.hdf5'.format(epoch+1))\n",
        "\n",
        "    print('_'*80)\n",
        "\n",
        "    # Reset metrics after every epoch\n",
        "    train_loss.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1\n",
            "Initializing from scratch.\n",
            "Saved checkpoint for epoch 1: tf_ckpts/ckpt-1\n",
            "Time taken  269.3935046195984\n",
            "Epoch 1, Loss: 1.319, Accuracy: 58.133, Val Loss: 1.334, Val Accuracy: 60.437\n",
            "Validation accuracy increased from 0.000 to 60.437. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  2\n",
            "Restored from tf_ckpts/ckpt-1\n",
            "Saved checkpoint for epoch 2: tf_ckpts/ckpt-2\n",
            "Time taken  11.983951807022095\n",
            "Epoch 2, Loss: 1.113, Accuracy: 64.587, Val Loss: 1.163, Val Accuracy: 63.476\n",
            "Validation accuracy increased from 60.437 to 63.476. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  3\n",
            "Restored from tf_ckpts/ckpt-2\n",
            "Saved checkpoint for epoch 3: tf_ckpts/ckpt-3\n",
            "Time taken  12.168817281723022\n",
            "Epoch 3, Loss: 1.020, Accuracy: 66.099, Val Loss: 1.106, Val Accuracy: 60.912\n",
            "________________________________________________________________________________\n",
            "Epoch  4\n",
            "Restored from tf_ckpts/ckpt-3\n",
            "Saved checkpoint for epoch 4: tf_ckpts/ckpt-4\n",
            "Time taken  12.079375982284546\n",
            "Epoch 4, Loss: 0.967, Accuracy: 66.801, Val Loss: 0.991, Val Accuracy: 64.125\n",
            "Validation accuracy increased from 63.476 to 64.125. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  5\n",
            "Restored from tf_ckpts/ckpt-4\n",
            "Saved checkpoint for epoch 5: tf_ckpts/ckpt-5\n",
            "Time taken  12.145480871200562\n",
            "Epoch 5, Loss: 0.930, Accuracy: 67.282, Val Loss: 0.976, Val Accuracy: 65.700\n",
            "Validation accuracy increased from 64.125 to 65.700. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  6\n",
            "Restored from tf_ckpts/ckpt-5\n",
            "Saved checkpoint for epoch 6: tf_ckpts/ckpt-6\n",
            "Time taken  11.991719245910645\n",
            "Epoch 6, Loss: 0.904, Accuracy: 67.630, Val Loss: 0.973, Val Accuracy: 63.604\n",
            "________________________________________________________________________________\n",
            "Epoch  7\n",
            "Restored from tf_ckpts/ckpt-6\n",
            "Saved checkpoint for epoch 7: tf_ckpts/ckpt-7\n",
            "Time taken  12.047407865524292\n",
            "Epoch 7, Loss: 0.886, Accuracy: 67.830, Val Loss: 0.946, Val Accuracy: 65.687\n",
            "________________________________________________________________________________\n",
            "Epoch  8\n",
            "Restored from tf_ckpts/ckpt-7\n",
            "Saved checkpoint for epoch 8: tf_ckpts/ckpt-8\n",
            "Time taken  11.92277479171753\n",
            "Epoch 8, Loss: 0.872, Accuracy: 68.024, Val Loss: 0.970, Val Accuracy: 63.708\n",
            "________________________________________________________________________________\n",
            "Epoch  9\n",
            "Restored from tf_ckpts/ckpt-8\n",
            "Saved checkpoint for epoch 9: tf_ckpts/ckpt-9\n",
            "Time taken  11.951654434204102\n",
            "Epoch 9, Loss: 0.859, Accuracy: 68.263, Val Loss: 0.985, Val Accuracy: 62.372\n",
            "________________________________________________________________________________\n",
            "Epoch  10\n",
            "Restored from tf_ckpts/ckpt-9\n",
            "Saved checkpoint for epoch 10: tf_ckpts/ckpt-10\n",
            "Time taken  12.062811374664307\n",
            "Epoch 10, Loss: 0.853, Accuracy: 68.296, Val Loss: 0.917, Val Accuracy: 65.586\n",
            "________________________________________________________________________________\n",
            "Epoch  11\n",
            "Restored from tf_ckpts/ckpt-10\n",
            "Saved checkpoint for epoch 11: tf_ckpts/ckpt-11\n",
            "Time taken  11.7898588180542\n",
            "Epoch 11, Loss: 0.845, Accuracy: 68.482, Val Loss: 1.005, Val Accuracy: 64.594\n",
            "________________________________________________________________________________\n",
            "Epoch  12\n",
            "Restored from tf_ckpts/ckpt-11\n",
            "Saved checkpoint for epoch 12: tf_ckpts/ckpt-12\n",
            "Time taken  11.817471742630005\n",
            "Epoch 12, Loss: 0.839, Accuracy: 68.579, Val Loss: 1.026, Val Accuracy: 62.317\n",
            "________________________________________________________________________________\n",
            "Epoch  13\n",
            "Restored from tf_ckpts/ckpt-12\n",
            "Saved checkpoint for epoch 13: tf_ckpts/ckpt-13\n",
            "Time taken  12.236025333404541\n",
            "Epoch 13, Loss: 0.834, Accuracy: 68.604, Val Loss: 0.996, Val Accuracy: 61.015\n",
            "________________________________________________________________________________\n",
            "Epoch  14\n",
            "Restored from tf_ckpts/ckpt-13\n",
            "Saved checkpoint for epoch 14: tf_ckpts/ckpt-14\n",
            "Time taken  11.770705699920654\n",
            "Epoch 14, Loss: 0.830, Accuracy: 68.608, Val Loss: 0.940, Val Accuracy: 64.346\n",
            "________________________________________________________________________________\n",
            "Epoch  15\n",
            "Restored from tf_ckpts/ckpt-14\n",
            "Saved checkpoint for epoch 15: tf_ckpts/ckpt-15\n",
            "Time taken  11.867155075073242\n",
            "Epoch 15, Loss: 0.823, Accuracy: 68.817, Val Loss: 1.046, Val Accuracy: 61.490\n",
            "________________________________________________________________________________\n",
            "Epoch  16\n",
            "Restored from tf_ckpts/ckpt-15\n",
            "Saved checkpoint for epoch 16: tf_ckpts/ckpt-16\n",
            "Time taken  11.701530933380127\n",
            "Epoch 16, Loss: 0.820, Accuracy: 68.879, Val Loss: 0.967, Val Accuracy: 62.073\n",
            "________________________________________________________________________________\n",
            "Epoch  17\n",
            "Restored from tf_ckpts/ckpt-16\n",
            "Saved checkpoint for epoch 17: tf_ckpts/ckpt-17\n",
            "Time taken  11.773823976516724\n",
            "Epoch 17, Loss: 0.820, Accuracy: 68.835, Val Loss: 1.094, Val Accuracy: 58.438\n",
            "________________________________________________________________________________\n",
            "Epoch  18\n",
            "Restored from tf_ckpts/ckpt-17\n",
            "Saved checkpoint for epoch 18: tf_ckpts/ckpt-18\n",
            "Time taken  12.091720581054688\n",
            "Epoch 18, Loss: 0.816, Accuracy: 68.922, Val Loss: 1.114, Val Accuracy: 58.737\n",
            "________________________________________________________________________________\n",
            "Epoch  19\n",
            "Restored from tf_ckpts/ckpt-18\n",
            "Saved checkpoint for epoch 19: tf_ckpts/ckpt-19\n",
            "Time taken  11.980301141738892\n",
            "Epoch 19, Loss: 0.810, Accuracy: 69.045, Val Loss: 1.008, Val Accuracy: 63.039\n",
            "________________________________________________________________________________\n",
            "Epoch  20\n",
            "Restored from tf_ckpts/ckpt-19\n",
            "Saved checkpoint for epoch 20: tf_ckpts/ckpt-20\n",
            "Time taken  11.919584512710571\n",
            "Epoch 20, Loss: 0.809, Accuracy: 69.074, Val Loss: 0.950, Val Accuracy: 65.661\n",
            "________________________________________________________________________________\n",
            "Epoch  21\n",
            "Restored from tf_ckpts/ckpt-20\n",
            "Saved checkpoint for epoch 21: tf_ckpts/ckpt-21\n",
            "Time taken  12.13401985168457\n",
            "Epoch 21, Loss: 0.807, Accuracy: 69.172, Val Loss: 0.911, Val Accuracy: 65.713\n",
            "Validation accuracy increased from 65.700 to 65.713. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  22\n",
            "Restored from tf_ckpts/ckpt-21\n",
            "Saved checkpoint for epoch 22: tf_ckpts/ckpt-22\n",
            "Time taken  12.141206741333008\n",
            "Epoch 22, Loss: 0.805, Accuracy: 69.223, Val Loss: 0.915, Val Accuracy: 64.751\n",
            "________________________________________________________________________________\n",
            "Epoch  23\n",
            "Restored from tf_ckpts/ckpt-22\n",
            "Saved checkpoint for epoch 23: tf_ckpts/ckpt-23\n",
            "Time taken  12.197123050689697\n",
            "Epoch 23, Loss: 0.803, Accuracy: 69.260, Val Loss: 0.882, Val Accuracy: 66.663\n",
            "Validation accuracy increased from 65.713 to 66.663. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  24\n",
            "Restored from tf_ckpts/ckpt-23\n",
            "Saved checkpoint for epoch 24: tf_ckpts/ckpt-24\n",
            "Time taken  12.069292783737183\n",
            "Epoch 24, Loss: 0.801, Accuracy: 69.266, Val Loss: 0.988, Val Accuracy: 62.111\n",
            "________________________________________________________________________________\n",
            "Epoch  25\n",
            "Restored from tf_ckpts/ckpt-24\n",
            "Saved checkpoint for epoch 25: tf_ckpts/ckpt-25\n",
            "Time taken  11.995481729507446\n",
            "Epoch 25, Loss: 0.801, Accuracy: 69.235, Val Loss: 1.029, Val Accuracy: 63.367\n",
            "________________________________________________________________________________\n",
            "Epoch  26\n",
            "Restored from tf_ckpts/ckpt-25\n",
            "Saved checkpoint for epoch 26: tf_ckpts/ckpt-26\n",
            "Time taken  11.81150484085083\n",
            "Epoch 26, Loss: 0.798, Accuracy: 69.285, Val Loss: 1.005, Val Accuracy: 59.901\n",
            "________________________________________________________________________________\n",
            "Epoch  27\n",
            "Restored from tf_ckpts/ckpt-26\n",
            "Saved checkpoint for epoch 27: tf_ckpts/ckpt-27\n",
            "Time taken  11.897507905960083\n",
            "Epoch 27, Loss: 0.795, Accuracy: 69.428, Val Loss: 0.947, Val Accuracy: 65.016\n",
            "________________________________________________________________________________\n",
            "Epoch  28\n",
            "Restored from tf_ckpts/ckpt-27\n",
            "Saved checkpoint for epoch 28: tf_ckpts/ckpt-28\n",
            "Time taken  11.87843942642212\n",
            "Epoch 28, Loss: 0.795, Accuracy: 69.434, Val Loss: 0.941, Val Accuracy: 65.806\n",
            "________________________________________________________________________________\n",
            "Epoch  29\n",
            "Restored from tf_ckpts/ckpt-28\n",
            "Saved checkpoint for epoch 29: tf_ckpts/ckpt-29\n",
            "Time taken  12.034092903137207\n",
            "Epoch 29, Loss: 0.795, Accuracy: 69.397, Val Loss: 0.962, Val Accuracy: 65.028\n",
            "________________________________________________________________________________\n",
            "Epoch  30\n",
            "Restored from tf_ckpts/ckpt-29\n",
            "Saved checkpoint for epoch 30: tf_ckpts/ckpt-30\n",
            "Time taken  11.98004674911499\n",
            "Epoch 30, Loss: 0.792, Accuracy: 69.497, Val Loss: 1.011, Val Accuracy: 61.756\n",
            "________________________________________________________________________________\n",
            "Epoch  31\n",
            "Restored from tf_ckpts/ckpt-30\n",
            "Saved checkpoint for epoch 31: tf_ckpts/ckpt-31\n",
            "Time taken  11.944332838058472\n",
            "Epoch 31, Loss: 0.791, Accuracy: 69.492, Val Loss: 1.069, Val Accuracy: 64.459\n",
            "________________________________________________________________________________\n",
            "Epoch  32\n",
            "Restored from tf_ckpts/ckpt-31\n",
            "Saved checkpoint for epoch 32: tf_ckpts/ckpt-32\n",
            "Time taken  12.114469528198242\n",
            "Epoch 32, Loss: 0.789, Accuracy: 69.587, Val Loss: 1.018, Val Accuracy: 64.224\n",
            "________________________________________________________________________________\n",
            "Epoch  33\n",
            "Restored from tf_ckpts/ckpt-32\n",
            "Saved checkpoint for epoch 33: tf_ckpts/ckpt-33\n",
            "Time taken  11.756131172180176\n",
            "Epoch 33, Loss: 0.788, Accuracy: 69.593, Val Loss: 1.044, Val Accuracy: 59.488\n",
            "________________________________________________________________________________\n",
            "Epoch  34\n",
            "Restored from tf_ckpts/ckpt-33\n",
            "Saved checkpoint for epoch 34: tf_ckpts/ckpt-34\n",
            "Time taken  11.680282354354858\n",
            "Epoch 34, Loss: 0.788, Accuracy: 69.602, Val Loss: 1.022, Val Accuracy: 60.412\n",
            "________________________________________________________________________________\n",
            "Epoch  35\n",
            "Restored from tf_ckpts/ckpt-34\n",
            "Saved checkpoint for epoch 35: tf_ckpts/ckpt-35\n",
            "Time taken  12.132677555084229\n",
            "Epoch 35, Loss: 0.787, Accuracy: 69.706, Val Loss: 1.005, Val Accuracy: 61.916\n",
            "________________________________________________________________________________\n",
            "Epoch  36\n",
            "Restored from tf_ckpts/ckpt-35\n",
            "Saved checkpoint for epoch 36: tf_ckpts/ckpt-36\n",
            "Time taken  11.934008359909058\n",
            "Epoch 36, Loss: 0.784, Accuracy: 69.706, Val Loss: 0.956, Val Accuracy: 63.734\n",
            "________________________________________________________________________________\n",
            "Epoch  37\n",
            "Restored from tf_ckpts/ckpt-36\n",
            "Saved checkpoint for epoch 37: tf_ckpts/ckpt-37\n",
            "Time taken  11.902929306030273\n",
            "Epoch 37, Loss: 0.784, Accuracy: 69.745, Val Loss: 1.027, Val Accuracy: 60.686\n",
            "________________________________________________________________________________\n",
            "Epoch  38\n",
            "Restored from tf_ckpts/ckpt-37\n",
            "Saved checkpoint for epoch 38: tf_ckpts/ckpt-38\n",
            "Time taken  12.026394367218018\n",
            "Epoch 38, Loss: 0.785, Accuracy: 69.644, Val Loss: 0.947, Val Accuracy: 63.904\n",
            "________________________________________________________________________________\n",
            "Epoch  39\n",
            "Restored from tf_ckpts/ckpt-38\n",
            "Saved checkpoint for epoch 39: tf_ckpts/ckpt-39\n",
            "Time taken  11.819307565689087\n",
            "Epoch 39, Loss: 0.782, Accuracy: 69.803, Val Loss: 0.957, Val Accuracy: 63.000\n",
            "________________________________________________________________________________\n",
            "Epoch  40\n",
            "Restored from tf_ckpts/ckpt-39\n",
            "Saved checkpoint for epoch 40: tf_ckpts/ckpt-40\n",
            "Time taken  11.76896333694458\n",
            "Epoch 40, Loss: 0.780, Accuracy: 69.841, Val Loss: 1.048, Val Accuracy: 63.705\n",
            "________________________________________________________________________________\n",
            "Epoch  41\n",
            "Restored from tf_ckpts/ckpt-40\n",
            "Saved checkpoint for epoch 41: tf_ckpts/ckpt-41\n",
            "Time taken  12.021905899047852\n",
            "Epoch 41, Loss: 0.782, Accuracy: 69.813, Val Loss: 0.876, Val Accuracy: 67.130\n",
            "Validation accuracy increased from 66.663 to 67.130. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  42\n",
            "Restored from tf_ckpts/ckpt-41\n",
            "Saved checkpoint for epoch 42: tf_ckpts/ckpt-42\n",
            "Time taken  11.663153409957886\n",
            "Epoch 42, Loss: 0.780, Accuracy: 69.815, Val Loss: 0.981, Val Accuracy: 62.816\n",
            "________________________________________________________________________________\n",
            "Epoch  43\n",
            "Restored from tf_ckpts/ckpt-42\n",
            "Saved checkpoint for epoch 43: tf_ckpts/ckpt-43\n",
            "Time taken  11.82885456085205\n",
            "Epoch 43, Loss: 0.778, Accuracy: 69.893, Val Loss: 0.905, Val Accuracy: 65.023\n",
            "________________________________________________________________________________\n",
            "Epoch  44\n",
            "Restored from tf_ckpts/ckpt-43\n",
            "Saved checkpoint for epoch 44: tf_ckpts/ckpt-44\n",
            "Time taken  11.92144227027893\n",
            "Epoch 44, Loss: 0.778, Accuracy: 69.865, Val Loss: 0.883, Val Accuracy: 66.239\n",
            "________________________________________________________________________________\n",
            "Epoch  45\n",
            "Restored from tf_ckpts/ckpt-44\n",
            "Saved checkpoint for epoch 45: tf_ckpts/ckpt-45\n",
            "Time taken  11.8882737159729\n",
            "Epoch 45, Loss: 0.779, Accuracy: 69.849, Val Loss: 0.943, Val Accuracy: 64.494\n",
            "________________________________________________________________________________\n",
            "Epoch  46\n",
            "Restored from tf_ckpts/ckpt-45\n",
            "Saved checkpoint for epoch 46: tf_ckpts/ckpt-46\n",
            "Time taken  11.595901727676392\n",
            "Epoch 46, Loss: 0.776, Accuracy: 69.912, Val Loss: 0.910, Val Accuracy: 66.177\n",
            "________________________________________________________________________________\n",
            "Epoch  47\n",
            "Restored from tf_ckpts/ckpt-46\n",
            "Saved checkpoint for epoch 47: tf_ckpts/ckpt-47\n",
            "Time taken  12.063458919525146\n",
            "Epoch 47, Loss: 0.776, Accuracy: 69.939, Val Loss: 1.363, Val Accuracy: 50.138\n",
            "________________________________________________________________________________\n",
            "Epoch  48\n",
            "Restored from tf_ckpts/ckpt-47\n",
            "Saved checkpoint for epoch 48: tf_ckpts/ckpt-48\n",
            "Time taken  11.82086992263794\n",
            "Epoch 48, Loss: 0.776, Accuracy: 69.918, Val Loss: 0.903, Val Accuracy: 66.641\n",
            "________________________________________________________________________________\n",
            "Epoch  49\n",
            "Restored from tf_ckpts/ckpt-48\n",
            "Saved checkpoint for epoch 49: tf_ckpts/ckpt-49\n",
            "Time taken  11.62861156463623\n",
            "Epoch 49, Loss: 0.775, Accuracy: 69.981, Val Loss: 1.384, Val Accuracy: 47.600\n",
            "________________________________________________________________________________\n",
            "Epoch  50\n",
            "Restored from tf_ckpts/ckpt-49\n",
            "Saved checkpoint for epoch 50: tf_ckpts/ckpt-50\n",
            "Time taken  11.688026428222656\n",
            "Epoch 50, Loss: 0.777, Accuracy: 69.878, Val Loss: 0.900, Val Accuracy: 66.159\n",
            "________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "uZ9eNKAh_4H7",
        "outputId": "1ea0da43-e601-4af2-db03-b374391f4808"
      },
      "source": [
        "from IPython.core.display import Image, display\n",
        "display(Image('From scratch.png'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "From scratch.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5w80STlIPu-"
      },
      "source": [
        "# Loading the weights of the best model\n",
        "model.load_weights('pspnet_weights-epoch-41.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcxMwQOMxW75"
      },
      "source": [
        "def predict(model,image_path):\n",
        "    \"\"\"\n",
        "    This function will take the model which is going to be used to predict the image and the image path of \n",
        "    the input image as inputs and predict the mask\n",
        "    It returns the true mask and predicted mask\n",
        "    \"\"\"\n",
        "    # Getting the datapoint\n",
        "    # This function will load the image and its annotation (mask) and return a dictionary.\n",
        "    datapoint = parse_image(image_path)\n",
        "    # Normalizing the resizing the datapoint\n",
        "    input_image,image_mask = load_image_test(datapoint)\n",
        "    # As the model takes input with 4 dimensions (batch_size, rows, columns, channels),\n",
        "    # and the shape of the input image is (rows, columns, channels)\n",
        "    # we will expand the first dimension so we will get the shape as  (1, rows, columns, channels)\n",
        "    img = tf.expand_dims(input_image, 0)\n",
        "    # Predicting the image by passing it to the model\n",
        "    prediction = model(img)\n",
        "    # The model will predict 8 outputs for each pixel\n",
        "    # We have to get the maximum value out of it\n",
        "    prediction = tf.argmax(prediction, axis=-1)\n",
        "    prediction = tf.squeeze(prediction, axis = 0)\n",
        "    pred_mask = tf.expand_dims(prediction, axis=-1)\n",
        "    # Displaying the input image, true mask, predicted mask\n",
        "    #display_sample([input_image, image_mask, pred_mask])\n",
        "    return image_mask, pred_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdb_IrorxkXN"
      },
      "source": [
        "# Reference -> https://github.com/saisandeepNSS/IDD_SemanticSegmentation\n",
        "\n",
        "def IoU(y_i,y_pred):\n",
        "    # This function calculates the mean Intersection over Union\n",
        "    # Mean IoU = TP/(FN + TP + FP)\n",
        "    # This list will save the IoU of all the classes\n",
        "    IoUs = []\n",
        "    # Defining the number of classes which the model has predicted\n",
        "    n_classes = 8\n",
        "    for c in range(n_classes):\n",
        "        # Calculating the True Positives\n",
        "        TP = np.sum((y_i == c)&(y_pred==c))\n",
        "        # Calculating the False Positives\n",
        "        FP = np.sum((y_i != c)&(y_pred==c))\n",
        "        # Calculating the False Negatives\n",
        "        FN = np.sum((y_i == c)&(y_pred!= c))\n",
        "        # Calculating the IoU for the particular class\n",
        "        IoU = TP/float(TP + FP + FN)\n",
        "        # Printing the outputs\n",
        "        # Uncomment the print statement below when you want to analyze the results for each class\n",
        "        #print(\"class {:02.0f}: #TP={:6.0f}, #FP={:6.0f}, #FN={:5.0f}, IoU={:4.3f}\".format(c,TP,FP,FN,IoU))\n",
        "        # Appending the IoU to the list as it mean needs to be calculated later\n",
        "        if(math.isnan(IoU)):\n",
        "            IoUs.append(0)\n",
        "            continue\n",
        "        IoUs.append(IoU)\n",
        "    # Calculating the mean\n",
        "    mIoU = np.mean(IoUs)\n",
        "    #print(\"_________________\")\n",
        "    #print(\"Mean IoU: {:4.3f}\".format(mIoU))\n",
        "    return mIoU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K0m_pvx0FiD"
      },
      "source": [
        "### Validation mIoU for PSPNet from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ERQK1g_RAk"
      },
      "source": [
        "img_val = dataset_path + 'leftImg8bit/val/'\n",
        "val_paths = glob(img_val+'*/*_image.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugUHIIqfz7du"
      },
      "source": [
        "mIoU = []\n",
        "for path in val_paths:\n",
        "    true_mask, pred_mask = predict(model,path)\n",
        "    mIoU.append(IoU(true_mask, pred_mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k43_RFJgz9Af",
        "outputId": "002044ee-5ae4-4014-e017-9b9debd71bd8"
      },
      "source": [
        "print(\"Validation mIoU = \",sum(mIoU)/len(mIoU))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation mIoU =  0.24356363447025353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx5HeG8i_4IG"
      },
      "source": [
        "# PSPNet with a pretrained Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAnfsJb7m_ka"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxbwYZt-_4II"
      },
      "source": [
        "# This class returns the pyramid feature map for Pyramid Pooling module\n",
        "# Pooling -> Convolution -> UpSampling\n",
        "class PyramidFeatureMap(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, block, filter_size, unsampling_size, pool_size, interpolation='bilinear'):\n",
        "        super(PyramidFeatureMap, self).__init__()\n",
        "        self.block = block\n",
        "        self.convolution_layer = Convolution2D(filter_size, kernel_size = (1,1))\n",
        "        self.upsampling_layer = UpSampling2D(unsampling_size, interpolation = interpolation)\n",
        "        self.average_pooling = AveragePooling2D(pool_size)\n",
        "        self.global_average_pooling = GlobalAveragePooling2D()\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        if(self.block=='red'):\n",
        "            pool_output = self.global_average_pooling(inputs)\n",
        "            pool_output = Reshape((1,1,2048))(pool_output)\n",
        "        else:\n",
        "            pool_output = self.average_pooling(inputs)\n",
        "        conv_output = self.convolution_layer(pool_output)\n",
        "        upsampling_output = self.upsampling_layer(conv_output)\n",
        "        return upsampling_output        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emNzzNxp_4IJ"
      },
      "source": [
        "# This class builds the Pyramid Pooling Module\n",
        "# It builds the 4 feature maps and then concatenates all of them with the input\n",
        "class PyramidPoolingModule(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(PyramidPoolingModule, self).__init__()\n",
        "        self.block_red = PyramidFeatureMap('red', 64, 8, (1,1))\n",
        "        self.block_yellow = PyramidFeatureMap('yellow', 64, 2, (2,2))\n",
        "        self.block_blue = PyramidFeatureMap('blue', 64, 4, (3,3))\n",
        "        self.block_green = PyramidFeatureMap('green', 64, 8, (6,6))\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        red_output = self.block_red(inputs)\n",
        "        yellow_output = self.block_yellow(inputs)\n",
        "        blue_output = self.block_blue(inputs)\n",
        "        green_output = self.block_green(inputs)\n",
        "        merged = concatenate([inputs, red_output, yellow_output, blue_output, green_output])\n",
        "        return merged  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fndfJxa0_4IK"
      },
      "source": [
        "# This block is for the last stage\n",
        "# It gets the output of Pyramid Pooling Module and then performs convolution operation\n",
        "# Pyramid Pooling -> Convolution -> Batch Normalization -> Softmax activation\n",
        "class DecoderBlock(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, num_classes):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.pyramid_pooling_module = PyramidPoolingModule()\n",
        "        self.convolution_layer = Convolution2D(num_classes, kernel_size = (3,3), padding = 'same')\n",
        "        self.batch_norm_layer = BatchNormalization()\n",
        "        self.activation = Activation('softmax')\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        merged = self.pyramid_pooling_module(inputs)\n",
        "        conv_output = self.convolution_layer(merged)\n",
        "        norm_output = self.batch_norm_layer(conv_output)\n",
        "        output = self.activation(norm_output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB-ASqEo_4IL"
      },
      "source": [
        "#This final class for the Model\n",
        "class PSPNetModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, num_classes):\n",
        "        super(PSPNetModel, self).__init__()\n",
        "        self.resnet = ResNet50(weights='imagenet', include_top = False, input_shape = (128,128,3))\n",
        "        for layer in self.resnet.layers:\n",
        "            layer.trainable = False\n",
        "        self.upsampling_encoder = UpSampling2D(size=(2,2), interpolation='bilinear')\n",
        "        self.upsampling = UpSampling2D(size=(16,16), interpolation='bilinear')\n",
        "        self.decoder = DecoderBlock(num_classes)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        encoder_output = self.resnet(inputs)\n",
        "        encoder_output = self.upsampling_encoder(encoder_output)\n",
        "        decoder_output = self.decoder(encoder_output)\n",
        "        final_output = self.upsampling(decoder_output)\n",
        "        return final_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFclA62p_4IM"
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "# SegmentationModel object\n",
        "model = PSPNetModel(N_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "499W0VQX_4IN"
      },
      "source": [
        "# Defining a loss object and an optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, 'tf_ckpts/', max_to_keep=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4NzAkAc_4IO"
      },
      "source": [
        "# Define the metrics\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K276SH9g_4IP"
      },
      "source": [
        "# Reference -> https://github.com/junhoning/machine_learning_tutorial/blob/b20b8a10438ec3e62f08f920744cc8ea854cde91/Visualization%20%26%20TensorBoard/%5BTensorBoard%5D%20Semantic%20Segmentation.ipynb\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, optimizer, x_train, y_train):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x_train, training=True)\n",
        "        loss = loss_object(y_train, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(y_train, predictions)\n",
        "    \n",
        "def train_and_checkpoint(model, manager, dataset, epoch):\n",
        "    '''\n",
        "    ckpt.restore(manager.latest_checkpoint)\n",
        "    if manager.latest_checkpoint:\n",
        "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "    else:\n",
        "        print(\"Initializing from scratch.\")'''\n",
        "    for (x_train, y_train) in dataset['train'].take(math.ceil(1403/16)):\n",
        "        train_step(model, optimizer, x_train, y_train)\n",
        "    ckpt.step.assign_add(1)\n",
        "    save_path = manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}: {}\".format(epoch, save_path))\n",
        "    \n",
        "@tf.function\n",
        "def test_step(model, x_test, y_test):\n",
        "    predictions = model(x_test)\n",
        "    loss = loss_object(y_test, predictions)\n",
        "    test_loss(loss)\n",
        "    test_accuracy(y_test, predictions)\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de5lgczK_4IP"
      },
      "source": [
        "train_log_dir = 'logs/gradient_tape/train'\n",
        "test_log_dir = 'logs/gradient_tape/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "scrolled": true,
        "id": "rCThXp4R_4IP",
        "outputId": "79b5f9c2-578e-40d9-b6fc-d9c2029607fb"
      },
      "source": [
        "# This variable will help to save the best model if its performance increases after an epoch   \n",
        "highest_accuracy = 0\n",
        "\n",
        "# Training the model for 50 epochs\n",
        "for epoch in range(50):\n",
        "\n",
        "    print(\"Epoch \",epoch+1)\n",
        "    \n",
        "    # Getting the current time before starting the training\n",
        "    # This will help to keep track of how much time an epoch took\n",
        "    start = time.time()\n",
        "    \n",
        "    train_and_checkpoint(model, manager, dataset, epoch+1)\n",
        "    \n",
        "    # Saving the train loss and train accuracy metric for TensorBoard visualization\n",
        "    with train_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', train_loss.result(), step=ckpt.step.numpy())\n",
        "        tf.summary.scalar('accuracy', train_accuracy.result(), step=ckpt.step.numpy())\n",
        "    \n",
        "    # Validation phase\n",
        "    for (x_test, y_test) in dataset['val'].take(math.ceil(204/16)):\n",
        "        pred = test_step(model, x_test, y_test)\n",
        "    \n",
        "    # Saving the validation loss and validation accuracy metric for Tensorboard visualization\n",
        "    with test_summary_writer.as_default():\n",
        "        tf.summary.scalar('loss', test_loss.result(), step=ckpt.step.numpy())\n",
        "        tf.summary.scalar('accuracy', test_accuracy.result(), step=ckpt.step.numpy())\n",
        "    \n",
        "    # Calculating the time it took for the entire epoch to run\n",
        "    print(\"Time taken \",time.time()-start)\n",
        "    \n",
        "    # Printing the metrics for the epoch\n",
        "    template = 'Epoch {}, Loss: {:.3f}, Accuracy: {:.3f}, Val Loss: {:.3f}, Val Accuracy: {:.3f}'\n",
        "    print (template.format(epoch+1,\n",
        "                            train_loss.result(), \n",
        "                            train_accuracy.result()*100,\n",
        "                            test_loss.result(), \n",
        "                            test_accuracy.result()*100))\n",
        "    \n",
        "    # If accuracy has increased in this epoch, updating the highest accuracy and saving the model\n",
        "    if(test_accuracy.result().numpy()*100>highest_accuracy):\n",
        "        print(\"Validation accuracy increased from {:.3f} to {:.3f}. Saving model weights.\".format(highest_accuracy,test_accuracy.result().numpy()*100))\n",
        "        highest_accuracy = test_accuracy.result().numpy()*100\n",
        "        model.save_weights('pspnet_weights-epoch-{}.hdf5'.format(epoch+1))\n",
        "\n",
        "    print('_'*80)\n",
        "\n",
        "    # Reset metrics after every epoch\n",
        "    train_loss.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  1\n",
            "Saved checkpoint for epoch 1: tf_ckpts/ckpt-1\n",
            "Time taken  11.591219425201416\n",
            "Epoch 1, Loss: 1.405, Accuracy: 55.695, Val Loss: 1.397, Val Accuracy: 54.426\n",
            "Validation accuracy increased from 0.000 to 54.426. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  2\n",
            "Saved checkpoint for epoch 2: tf_ckpts/ckpt-2\n",
            "Time taken  7.210581541061401\n",
            "Epoch 2, Loss: 1.222, Accuracy: 62.237, Val Loss: 1.185, Val Accuracy: 61.691\n",
            "Validation accuracy increased from 54.426 to 61.691. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  3\n",
            "Saved checkpoint for epoch 3: tf_ckpts/ckpt-3\n",
            "Time taken  7.408657789230347\n",
            "Epoch 3, Loss: 1.139, Accuracy: 64.068, Val Loss: 1.180, Val Accuracy: 63.452\n",
            "Validation accuracy increased from 61.691 to 63.452. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  4\n",
            "Saved checkpoint for epoch 4: tf_ckpts/ckpt-4\n",
            "Time taken  7.264097452163696\n",
            "Epoch 4, Loss: 1.080, Accuracy: 65.252, Val Loss: 1.154, Val Accuracy: 63.595\n",
            "Validation accuracy increased from 63.452 to 63.595. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  5\n",
            "Saved checkpoint for epoch 5: tf_ckpts/ckpt-5\n",
            "Time taken  7.329670190811157\n",
            "Epoch 5, Loss: 1.039, Accuracy: 65.797, Val Loss: 1.063, Val Accuracy: 63.552\n",
            "________________________________________________________________________________\n",
            "Epoch  6\n",
            "Saved checkpoint for epoch 6: tf_ckpts/ckpt-6\n",
            "Time taken  7.418422222137451\n",
            "Epoch 6, Loss: 1.006, Accuracy: 66.209, Val Loss: 1.083, Val Accuracy: 62.378\n",
            "________________________________________________________________________________\n",
            "Epoch  7\n",
            "Saved checkpoint for epoch 7: tf_ckpts/ckpt-7\n",
            "Time taken  7.1792120933532715\n",
            "Epoch 7, Loss: 0.974, Accuracy: 66.838, Val Loss: 1.029, Val Accuracy: 65.706\n",
            "Validation accuracy increased from 63.595 to 65.706. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  8\n",
            "Saved checkpoint for epoch 8: tf_ckpts/ckpt-8\n",
            "Time taken  7.2384421825408936\n",
            "Epoch 8, Loss: 0.956, Accuracy: 66.988, Val Loss: 0.988, Val Accuracy: 64.327\n",
            "________________________________________________________________________________\n",
            "Epoch  9\n",
            "Saved checkpoint for epoch 9: tf_ckpts/ckpt-9\n",
            "Time taken  7.0959413051605225\n",
            "Epoch 9, Loss: 0.938, Accuracy: 67.150, Val Loss: 1.091, Val Accuracy: 59.880\n",
            "________________________________________________________________________________\n",
            "Epoch  10\n",
            "Saved checkpoint for epoch 10: tf_ckpts/ckpt-10\n",
            "Time taken  7.29752254486084\n",
            "Epoch 10, Loss: 0.919, Accuracy: 67.624, Val Loss: 0.954, Val Accuracy: 64.914\n",
            "________________________________________________________________________________\n",
            "Epoch  11\n",
            "Saved checkpoint for epoch 11: tf_ckpts/ckpt-11\n",
            "Time taken  7.367499351501465\n",
            "Epoch 11, Loss: 0.910, Accuracy: 67.527, Val Loss: 0.962, Val Accuracy: 64.482\n",
            "________________________________________________________________________________\n",
            "Epoch  12\n",
            "Saved checkpoint for epoch 12: tf_ckpts/ckpt-12\n",
            "Time taken  7.266934394836426\n",
            "Epoch 12, Loss: 0.900, Accuracy: 67.691, Val Loss: 0.941, Val Accuracy: 66.465\n",
            "Validation accuracy increased from 65.706 to 66.465. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  13\n",
            "Saved checkpoint for epoch 13: tf_ckpts/ckpt-13\n",
            "Time taken  7.2067670822143555\n",
            "Epoch 13, Loss: 0.892, Accuracy: 67.766, Val Loss: 0.989, Val Accuracy: 62.914\n",
            "________________________________________________________________________________\n",
            "Epoch  14\n",
            "Saved checkpoint for epoch 14: tf_ckpts/ckpt-14\n",
            "Time taken  7.286325931549072\n",
            "Epoch 14, Loss: 0.883, Accuracy: 67.825, Val Loss: 1.156, Val Accuracy: 55.867\n",
            "________________________________________________________________________________\n",
            "Epoch  15\n",
            "Saved checkpoint for epoch 15: tf_ckpts/ckpt-15\n",
            "Time taken  7.212453365325928\n",
            "Epoch 15, Loss: 0.877, Accuracy: 67.916, Val Loss: 0.926, Val Accuracy: 65.969\n",
            "________________________________________________________________________________\n",
            "Epoch  16\n",
            "Saved checkpoint for epoch 16: tf_ckpts/ckpt-16\n",
            "Time taken  7.184956312179565\n",
            "Epoch 16, Loss: 0.865, Accuracy: 68.303, Val Loss: 1.096, Val Accuracy: 58.883\n",
            "________________________________________________________________________________\n",
            "Epoch  17\n",
            "Saved checkpoint for epoch 17: tf_ckpts/ckpt-17\n",
            "Time taken  7.304548978805542\n",
            "Epoch 17, Loss: 0.862, Accuracy: 68.212, Val Loss: 0.916, Val Accuracy: 65.641\n",
            "________________________________________________________________________________\n",
            "Epoch  18\n",
            "Saved checkpoint for epoch 18: tf_ckpts/ckpt-18\n",
            "Time taken  7.139010906219482\n",
            "Epoch 18, Loss: 0.859, Accuracy: 68.209, Val Loss: 0.957, Val Accuracy: 65.141\n",
            "________________________________________________________________________________\n",
            "Epoch  19\n",
            "Saved checkpoint for epoch 19: tf_ckpts/ckpt-19\n",
            "Time taken  7.354763507843018\n",
            "Epoch 19, Loss: 0.856, Accuracy: 68.256, Val Loss: 0.979, Val Accuracy: 64.364\n",
            "________________________________________________________________________________\n",
            "Epoch  20\n",
            "Saved checkpoint for epoch 20: tf_ckpts/ckpt-20\n",
            "Time taken  7.2101099491119385\n",
            "Epoch 20, Loss: 0.850, Accuracy: 68.377, Val Loss: 1.033, Val Accuracy: 59.610\n",
            "________________________________________________________________________________\n",
            "Epoch  21\n",
            "Saved checkpoint for epoch 21: tf_ckpts/ckpt-21\n",
            "Time taken  7.285935163497925\n",
            "Epoch 21, Loss: 0.848, Accuracy: 68.308, Val Loss: 0.976, Val Accuracy: 64.981\n",
            "________________________________________________________________________________\n",
            "Epoch  22\n",
            "Saved checkpoint for epoch 22: tf_ckpts/ckpt-22\n",
            "Time taken  7.576024055480957\n",
            "Epoch 22, Loss: 0.845, Accuracy: 68.343, Val Loss: 0.946, Val Accuracy: 64.652\n",
            "________________________________________________________________________________\n",
            "Epoch  23\n",
            "Saved checkpoint for epoch 23: tf_ckpts/ckpt-23\n",
            "Time taken  7.225688219070435\n",
            "Epoch 23, Loss: 0.840, Accuracy: 68.599, Val Loss: 0.910, Val Accuracy: 66.329\n",
            "________________________________________________________________________________\n",
            "Epoch  24\n",
            "Saved checkpoint for epoch 24: tf_ckpts/ckpt-24\n",
            "Time taken  7.403748512268066\n",
            "Epoch 24, Loss: 0.834, Accuracy: 68.673, Val Loss: 0.960, Val Accuracy: 63.957\n",
            "________________________________________________________________________________\n",
            "Epoch  25\n",
            "Saved checkpoint for epoch 25: tf_ckpts/ckpt-25\n",
            "Time taken  7.300802946090698\n",
            "Epoch 25, Loss: 0.836, Accuracy: 68.520, Val Loss: 0.969, Val Accuracy: 64.513\n",
            "________________________________________________________________________________\n",
            "Epoch  26\n",
            "Saved checkpoint for epoch 26: tf_ckpts/ckpt-26\n",
            "Time taken  7.010425806045532\n",
            "Epoch 26, Loss: 0.832, Accuracy: 68.690, Val Loss: 0.939, Val Accuracy: 64.732\n",
            "________________________________________________________________________________\n",
            "Epoch  27\n",
            "Saved checkpoint for epoch 27: tf_ckpts/ckpt-27\n",
            "Time taken  7.0638203620910645\n",
            "Epoch 27, Loss: 0.830, Accuracy: 68.710, Val Loss: 0.898, Val Accuracy: 64.904\n",
            "________________________________________________________________________________\n",
            "Epoch  28\n",
            "Saved checkpoint for epoch 28: tf_ckpts/ckpt-28\n",
            "Time taken  7.2963502407073975\n",
            "Epoch 28, Loss: 0.826, Accuracy: 68.756, Val Loss: 1.001, Val Accuracy: 63.073\n",
            "________________________________________________________________________________\n",
            "Epoch  29\n",
            "Saved checkpoint for epoch 29: tf_ckpts/ckpt-29\n",
            "Time taken  7.24605655670166\n",
            "Epoch 29, Loss: 0.826, Accuracy: 68.845, Val Loss: 0.909, Val Accuracy: 65.645\n",
            "________________________________________________________________________________\n",
            "Epoch  30\n",
            "Saved checkpoint for epoch 30: tf_ckpts/ckpt-30\n",
            "Time taken  7.17319393157959\n",
            "Epoch 30, Loss: 0.821, Accuracy: 68.995, Val Loss: 0.929, Val Accuracy: 65.997\n",
            "________________________________________________________________________________\n",
            "Epoch  31\n",
            "Saved checkpoint for epoch 31: tf_ckpts/ckpt-31\n",
            "Time taken  7.280449628829956\n",
            "Epoch 31, Loss: 0.822, Accuracy: 68.836, Val Loss: 0.953, Val Accuracy: 64.327\n",
            "________________________________________________________________________________\n",
            "Epoch  32\n",
            "Saved checkpoint for epoch 32: tf_ckpts/ckpt-32\n",
            "Time taken  7.006879568099976\n",
            "Epoch 32, Loss: 0.817, Accuracy: 69.009, Val Loss: 0.992, Val Accuracy: 66.069\n",
            "________________________________________________________________________________\n",
            "Epoch  33\n",
            "Saved checkpoint for epoch 33: tf_ckpts/ckpt-33\n",
            "Time taken  7.247586965560913\n",
            "Epoch 33, Loss: 0.818, Accuracy: 68.933, Val Loss: 0.965, Val Accuracy: 63.335\n",
            "________________________________________________________________________________\n",
            "Epoch  34\n",
            "Saved checkpoint for epoch 34: tf_ckpts/ckpt-34\n",
            "Time taken  7.104877233505249\n",
            "Epoch 34, Loss: 0.821, Accuracy: 68.768, Val Loss: 0.964, Val Accuracy: 63.067\n",
            "________________________________________________________________________________\n",
            "Epoch  35\n",
            "Saved checkpoint for epoch 35: tf_ckpts/ckpt-35\n",
            "Time taken  7.241483449935913\n",
            "Epoch 35, Loss: 0.814, Accuracy: 69.068, Val Loss: 1.077, Val Accuracy: 63.928\n",
            "________________________________________________________________________________\n",
            "Epoch  36\n",
            "Saved checkpoint for epoch 36: tf_ckpts/ckpt-36\n",
            "Time taken  7.162634611129761\n",
            "Epoch 36, Loss: 0.816, Accuracy: 68.906, Val Loss: 0.927, Val Accuracy: 64.988\n",
            "________________________________________________________________________________\n",
            "Epoch  37\n",
            "Saved checkpoint for epoch 37: tf_ckpts/ckpt-37\n",
            "Time taken  7.372175455093384\n",
            "Epoch 37, Loss: 0.812, Accuracy: 69.114, Val Loss: 0.891, Val Accuracy: 66.223\n",
            "________________________________________________________________________________\n",
            "Epoch  38\n",
            "Saved checkpoint for epoch 38: tf_ckpts/ckpt-38\n",
            "Time taken  7.255686044692993\n",
            "Epoch 38, Loss: 0.810, Accuracy: 69.138, Val Loss: 0.991, Val Accuracy: 62.219\n",
            "________________________________________________________________________________\n",
            "Epoch  39\n",
            "Saved checkpoint for epoch 39: tf_ckpts/ckpt-39\n",
            "Time taken  7.31021785736084\n",
            "Epoch 39, Loss: 0.811, Accuracy: 69.011, Val Loss: 1.085, Val Accuracy: 63.773\n",
            "________________________________________________________________________________\n",
            "Epoch  40\n",
            "Saved checkpoint for epoch 40: tf_ckpts/ckpt-40\n",
            "Time taken  7.305400848388672\n",
            "Epoch 40, Loss: 0.807, Accuracy: 69.228, Val Loss: 0.907, Val Accuracy: 66.654\n",
            "Validation accuracy increased from 66.465 to 66.654. Saving model weights.\n",
            "________________________________________________________________________________\n",
            "Epoch  41\n",
            "Saved checkpoint for epoch 41: tf_ckpts/ckpt-41\n",
            "Time taken  7.136502981185913\n",
            "Epoch 41, Loss: 0.808, Accuracy: 69.203, Val Loss: 1.068, Val Accuracy: 61.955\n",
            "________________________________________________________________________________\n",
            "Epoch  42\n",
            "Saved checkpoint for epoch 42: tf_ckpts/ckpt-42\n",
            "Time taken  7.324802875518799\n",
            "Epoch 42, Loss: 0.805, Accuracy: 69.141, Val Loss: 0.978, Val Accuracy: 65.293\n",
            "________________________________________________________________________________\n",
            "Epoch  43\n",
            "Saved checkpoint for epoch 43: tf_ckpts/ckpt-43\n",
            "Time taken  7.171260595321655\n",
            "Epoch 43, Loss: 0.805, Accuracy: 69.147, Val Loss: 0.942, Val Accuracy: 62.945\n",
            "________________________________________________________________________________\n",
            "Epoch  44\n",
            "Saved checkpoint for epoch 44: tf_ckpts/ckpt-44\n",
            "Time taken  7.219796419143677\n",
            "Epoch 44, Loss: 0.804, Accuracy: 69.146, Val Loss: 1.552, Val Accuracy: 42.245\n",
            "________________________________________________________________________________\n",
            "Epoch  45\n",
            "Saved checkpoint for epoch 45: tf_ckpts/ckpt-45\n",
            "Time taken  7.327098369598389\n",
            "Epoch 45, Loss: 0.803, Accuracy: 69.287, Val Loss: 0.936, Val Accuracy: 61.746\n",
            "________________________________________________________________________________\n",
            "Epoch  46\n",
            "Saved checkpoint for epoch 46: tf_ckpts/ckpt-46\n",
            "Time taken  7.3458778858184814\n",
            "Epoch 46, Loss: 0.801, Accuracy: 69.297, Val Loss: 0.892, Val Accuracy: 64.713\n",
            "________________________________________________________________________________\n",
            "Epoch  47\n",
            "Saved checkpoint for epoch 47: tf_ckpts/ckpt-47\n",
            "Time taken  7.232350587844849\n",
            "Epoch 47, Loss: 0.801, Accuracy: 69.298, Val Loss: 0.936, Val Accuracy: 63.779\n",
            "________________________________________________________________________________\n",
            "Epoch  48\n",
            "Saved checkpoint for epoch 48: tf_ckpts/ckpt-48\n",
            "Time taken  7.501100063323975\n",
            "Epoch 48, Loss: 0.805, Accuracy: 69.176, Val Loss: 1.260, Val Accuracy: 61.539\n",
            "________________________________________________________________________________\n",
            "Epoch  49\n",
            "Saved checkpoint for epoch 49: tf_ckpts/ckpt-49\n",
            "Time taken  7.382242918014526\n",
            "Epoch 49, Loss: 0.801, Accuracy: 69.260, Val Loss: 0.927, Val Accuracy: 63.075\n",
            "________________________________________________________________________________\n",
            "Epoch  50\n",
            "Saved checkpoint for epoch 50: tf_ckpts/ckpt-50\n",
            "Time taken  7.142063617706299\n",
            "Epoch 50, Loss: 0.802, Accuracy: 69.122, Val Loss: 1.044, Val Accuracy: 60.211\n",
            "________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "hVH-7L79_4IQ",
        "outputId": "80409d24-c0e3-4656-8275-a74ceae0b71d"
      },
      "source": [
        "display(Image('Pretrained Encoder.png'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "Pretrained Encoder.png",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKLs4Yyu_4IR"
      },
      "source": [
        "# Loading the weights of the best model\n",
        "model.load_weights('pspnet_weights-epoch-40.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA-SKwNz_4IR"
      },
      "source": [
        "def predict(model,image_path):\n",
        "    \"\"\"\n",
        "    This function will take the model which is going to be used to predict the image and the image path of \n",
        "    the input image as inputs and predict the mask\n",
        "    It returns the true mask and predicted mask\n",
        "    \"\"\"\n",
        "    # Getting the datapoint\n",
        "    # This function will load the image and its annotation (mask) and return a dictionary.\n",
        "    datapoint = parse_image(image_path)\n",
        "    # Normalizing the resizing the datapoint\n",
        "    input_image,image_mask = load_image_test(datapoint)\n",
        "    # As the model takes input with 4 dimensions (batch_size, rows, columns, channels),\n",
        "    # and the shape of the input image is (rows, columns, channels)\n",
        "    # we will expand the first dimension so we will get the shape as  (1, rows, columns, channels)\n",
        "    img = tf.expand_dims(input_image, 0)\n",
        "    # Predicting the image by passing it to the model\n",
        "    prediction = model(img)\n",
        "    # The model will predict 8 outputs for each pixel\n",
        "    # We have to get the maximum value out of it\n",
        "    prediction = tf.argmax(prediction, axis=-1)\n",
        "    prediction = tf.squeeze(prediction, axis = 0)\n",
        "    pred_mask = tf.expand_dims(prediction, axis=-1)\n",
        "    # Displaying the input image, true mask, predicted mask\n",
        "    #display_sample([input_image, image_mask, pred_mask])\n",
        "    return image_mask, pred_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPdg9dkK_4IS"
      },
      "source": [
        "# Reference -> https://github.com/saisandeepNSS/IDD_SemanticSegmentation\n",
        "\n",
        "def IoU(y_i,y_pred):\n",
        "    # This function calculates the mean Intersection over Union\n",
        "    # Mean IoU = TP/(FN + TP + FP)\n",
        "    # This list will save the IoU of all the classes\n",
        "    IoUs = []\n",
        "    # Defining the number of classes which the model has predicted\n",
        "    n_classes = 8\n",
        "    for c in range(n_classes):\n",
        "        # Calculating the True Positives\n",
        "        TP = np.sum((y_i == c)&(y_pred==c))\n",
        "        # Calculating the False Positives\n",
        "        FP = np.sum((y_i != c)&(y_pred==c))\n",
        "        # Calculating the False Negatives\n",
        "        FN = np.sum((y_i == c)&(y_pred!= c))\n",
        "        # Calculating the IoU for the particular class\n",
        "        IoU = TP/float(TP + FP + FN)\n",
        "        # Printing the outputs\n",
        "        # Uncomment the print statement below when you want to analyze the results for each class\n",
        "        #print(\"class {:02.0f}: #TP={:6.0f}, #FP={:6.0f}, #FN={:5.0f}, IoU={:4.3f}\".format(c,TP,FP,FN,IoU))\n",
        "        # Appending the IoU to the list as it mean needs to be calculated later\n",
        "        if(math.isnan(IoU)):\n",
        "            IoUs.append(0)\n",
        "            continue\n",
        "        IoUs.append(IoU)\n",
        "    # Calculating the mean\n",
        "    mIoU = np.mean(IoUs)\n",
        "    #print(\"_________________\")\n",
        "    #print(\"Mean IoU: {:4.3f}\".format(mIoU))\n",
        "    return mIoU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlaUSTF3_4IS"
      },
      "source": [
        "### Validation mIoU for PSPNet with Pretrained Encoder  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiQfsPKB_4IT"
      },
      "source": [
        "img_val = dataset_path + 'leftImg8bit/val/'\n",
        "val_paths = glob(img_val+'*/*_image.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G84FVntz_4IT"
      },
      "source": [
        "mIoU = []\n",
        "for path in val_paths:\n",
        "    true_mask, pred_mask = predict(model,path)\n",
        "    mIoU.append(IoU(true_mask, pred_mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bsXIkxg_4IT",
        "outputId": "d892955e-b2ee-4781-9798-07c64ec9c728"
      },
      "source": [
        "print(\"Validation mIoU = \",sum(mIoU)/len(mIoU))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation mIoU =  0.24133502200990242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDPZazRz_4IT"
      },
      "source": [
        "# Importing PSPNet from existing official models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4goO_tnLuNAb"
      },
      "source": [
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "for i in tqdm(train_paths):\n",
        "    img = i.split('/')[-1]\n",
        "    shutil.copy(i,'dataset_train/'+img)\n",
        "    mask_path = tf.strings.regex_replace(i, \"leftImg8bit\", \"gtFine\")\n",
        "    mask_path = tf.strings.regex_replace(mask_path, \"_image.jpg\", \"_label.png\")\n",
        "    mask = img.split('_')[0]+'_label.png'\n",
        "    shutil.copy(mask_path.numpy(),'dataset_train/'+mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OeblsmaAuY2Q",
        "outputId": "23c5b7ee-bf35-4af6-ac37-dec100eab1ef"
      },
      "source": [
        "for i in tqdm(val_paths):\n",
        "    img = i.split('/')[-1]\n",
        "    shutil.copy(i,'dataset_val/'+img)\n",
        "    mask_path = tf.strings.regex_replace(i, \"leftImg8bit\", \"gtFine\")\n",
        "    mask_path = tf.strings.regex_replace(mask_path, \"_image.jpg\", \"_label.png\")\n",
        "    mask = img.split('_')[0]+'_label.png'\n",
        "    shutil.copy(mask_path.numpy(),'dataset_val/'+mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 204/204 [04:46<00:00,  1.40s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stOBgVx1tqVm"
      },
      "source": [
        "# here dir_path is the route directory where all the images and segmentation maps are there\n",
        "train_path = \"dataset_train\"\n",
        "X_train = []\n",
        "for i in os.listdir(train_path):\n",
        "    X_train.append(i.split('.')[0])\n",
        "\n",
        "val_path = \"dataset_val\"\n",
        "X_test = []\n",
        "for i in os.listdir(val_path):\n",
        "    X_test.append(i.split('.')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GbzLx6nQbuoR",
        "scrolled": true,
        "outputId": "75b2397d-04cb-4607-9276-c91d4d7890f5"
      },
      "source": [
        "!pip install segmentation-models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: segmentation-models in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: efficientnet==1.0.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: image-classifiers==1.0.0 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.0)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.6/dist-packages (from segmentation-models) (1.0.8)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from efficientnet==1.0.0->segmentation-models) (0.16.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.18.4)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (3.2.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->efficientnet==1.0.0->segmentation-models) (2.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation-models) (1.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation-models) (2.8.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation-models) (4.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeJCeGIrbuoS"
      },
      "source": [
        "# we are importing the pretrained unet from the segmentation models\n",
        "# https://github.com/qubvel/segmentation_models\n",
        "import segmentation_models as sm\n",
        "from segmentation_models import PSPNet\n",
        "# sm.set_framework('tf.keras')\n",
        "tf.keras.backend.set_image_data_format('channels_last')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpTKEnXnbuoU",
        "scrolled": true
      },
      "source": [
        "# loading the unet model and using the resnet 34 and initilized weights with imagenet weights\n",
        "# \"classes\" :different types of classes in the dataset\n",
        "model = PSPNet('resnet34', encoder_weights='imagenet', classes=8, activation='softmax', input_shape=(192,192,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IhV3Ir5ibuoW",
        "outputId": "16b600d9-8c61-432d-f9c6-3e591f02d108"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "data (InputLayer)               (None, 192, 192, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bn_data (BatchNormalization)    (None, 192, 192, 3)  9           data[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_35 (ZeroPadding2 (None, 198, 198, 3)  0           bn_data[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv0 (Conv2D)                  (None, 96, 96, 64)   9408        zero_padding2d_35[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "bn0 (BatchNormalization)        (None, 96, 96, 64)   256         conv0[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "relu0 (Activation)              (None, 96, 96, 64)   0           bn0[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_36 (ZeroPadding2 (None, 98, 98, 64)   0           relu0[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "pooling0 (MaxPooling2D)         (None, 48, 48, 64)   0           zero_padding2d_36[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_bn1 (BatchNormaliz (None, 48, 48, 64)   256         pooling0[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_relu1 (Activation) (None, 48, 48, 64)   0           stage1_unit1_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_37 (ZeroPadding2 (None, 50, 50, 64)   0           stage1_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_conv1 (Conv2D)     (None, 48, 48, 64)   36864       zero_padding2d_37[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_bn2 (BatchNormaliz (None, 48, 48, 64)   256         stage1_unit1_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_relu2 (Activation) (None, 48, 48, 64)   0           stage1_unit1_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_38 (ZeroPadding2 (None, 50, 50, 64)   0           stage1_unit1_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_conv2 (Conv2D)     (None, 48, 48, 64)   36864       zero_padding2d_38[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit1_sc (Conv2D)        (None, 48, 48, 64)   4096        stage1_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 48, 48, 64)   0           stage1_unit1_conv2[0][0]         \n",
            "                                                                 stage1_unit1_sc[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_bn1 (BatchNormaliz (None, 48, 48, 64)   256         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_relu1 (Activation) (None, 48, 48, 64)   0           stage1_unit2_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_39 (ZeroPadding2 (None, 50, 50, 64)   0           stage1_unit2_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_conv1 (Conv2D)     (None, 48, 48, 64)   36864       zero_padding2d_39[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_bn2 (BatchNormaliz (None, 48, 48, 64)   256         stage1_unit2_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_relu2 (Activation) (None, 48, 48, 64)   0           stage1_unit2_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_40 (ZeroPadding2 (None, 50, 50, 64)   0           stage1_unit2_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit2_conv2 (Conv2D)     (None, 48, 48, 64)   36864       zero_padding2d_40[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 48, 48, 64)   0           stage1_unit2_conv2[0][0]         \n",
            "                                                                 add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_bn1 (BatchNormaliz (None, 48, 48, 64)   256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_relu1 (Activation) (None, 48, 48, 64)   0           stage1_unit3_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_41 (ZeroPadding2 (None, 50, 50, 64)   0           stage1_unit3_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_conv1 (Conv2D)     (None, 48, 48, 64)   36864       zero_padding2d_41[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_bn2 (BatchNormaliz (None, 48, 48, 64)   256         stage1_unit3_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_relu2 (Activation) (None, 48, 48, 64)   0           stage1_unit3_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_42 (ZeroPadding2 (None, 50, 50, 64)   0           stage1_unit3_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage1_unit3_conv2 (Conv2D)     (None, 48, 48, 64)   36864       zero_padding2d_42[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 48, 48, 64)   0           stage1_unit3_conv2[0][0]         \n",
            "                                                                 add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_bn1 (BatchNormaliz (None, 48, 48, 64)   256         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_relu1 (Activation) (None, 48, 48, 64)   0           stage2_unit1_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_43 (ZeroPadding2 (None, 50, 50, 64)   0           stage2_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_conv1 (Conv2D)     (None, 24, 24, 128)  73728       zero_padding2d_43[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_bn2 (BatchNormaliz (None, 24, 24, 128)  512         stage2_unit1_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_relu2 (Activation) (None, 24, 24, 128)  0           stage2_unit1_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_44 (ZeroPadding2 (None, 26, 26, 128)  0           stage2_unit1_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_conv2 (Conv2D)     (None, 24, 24, 128)  147456      zero_padding2d_44[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit1_sc (Conv2D)        (None, 24, 24, 128)  8192        stage2_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 24, 24, 128)  0           stage2_unit1_conv2[0][0]         \n",
            "                                                                 stage2_unit1_sc[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_bn1 (BatchNormaliz (None, 24, 24, 128)  512         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_relu1 (Activation) (None, 24, 24, 128)  0           stage2_unit2_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_45 (ZeroPadding2 (None, 26, 26, 128)  0           stage2_unit2_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_conv1 (Conv2D)     (None, 24, 24, 128)  147456      zero_padding2d_45[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_bn2 (BatchNormaliz (None, 24, 24, 128)  512         stage2_unit2_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_relu2 (Activation) (None, 24, 24, 128)  0           stage2_unit2_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_46 (ZeroPadding2 (None, 26, 26, 128)  0           stage2_unit2_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit2_conv2 (Conv2D)     (None, 24, 24, 128)  147456      zero_padding2d_46[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 24, 24, 128)  0           stage2_unit2_conv2[0][0]         \n",
            "                                                                 add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_bn1 (BatchNormaliz (None, 24, 24, 128)  512         add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_relu1 (Activation) (None, 24, 24, 128)  0           stage2_unit3_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_47 (ZeroPadding2 (None, 26, 26, 128)  0           stage2_unit3_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_conv1 (Conv2D)     (None, 24, 24, 128)  147456      zero_padding2d_47[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_bn2 (BatchNormaliz (None, 24, 24, 128)  512         stage2_unit3_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_relu2 (Activation) (None, 24, 24, 128)  0           stage2_unit3_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_48 (ZeroPadding2 (None, 26, 26, 128)  0           stage2_unit3_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit3_conv2 (Conv2D)     (None, 24, 24, 128)  147456      zero_padding2d_48[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 24, 24, 128)  0           stage2_unit3_conv2[0][0]         \n",
            "                                                                 add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_bn1 (BatchNormaliz (None, 24, 24, 128)  512         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_relu1 (Activation) (None, 24, 24, 128)  0           stage2_unit4_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_49 (ZeroPadding2 (None, 26, 26, 128)  0           stage2_unit4_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_conv1 (Conv2D)     (None, 24, 24, 128)  147456      zero_padding2d_49[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_bn2 (BatchNormaliz (None, 24, 24, 128)  512         stage2_unit4_conv1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_relu2 (Activation) (None, 24, 24, 128)  0           stage2_unit4_bn2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_50 (ZeroPadding2 (None, 26, 26, 128)  0           stage2_unit4_relu2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "stage2_unit4_conv2 (Conv2D)     (None, 24, 24, 128)  147456      zero_padding2d_50[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 24, 24, 128)  0           stage2_unit4_conv2[0][0]         \n",
            "                                                                 add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit1_bn1 (BatchNormaliz (None, 24, 24, 128)  512         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "stage3_unit1_relu1 (Activation) (None, 24, 24, 128)  0           stage3_unit1_bn1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "psp_level1_pooling (AveragePool (None, 1, 1, 128)    0           stage3_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "psp_level2_pooling (AveragePool (None, 2, 2, 128)    0           stage3_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "psp_level3_pooling (AveragePool (None, 3, 3, 128)    0           stage3_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "psp_level6_pooling (AveragePool (None, 6, 6, 128)    0           stage3_unit1_relu1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "psp_level1_conv (Conv2D)        (None, 1, 1, 512)    65536       psp_level1_pooling[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "psp_level2_conv (Conv2D)        (None, 2, 2, 512)    65536       psp_level2_pooling[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "psp_level3_conv (Conv2D)        (None, 3, 3, 512)    65536       psp_level3_pooling[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "psp_level6_conv (Conv2D)        (None, 6, 6, 512)    65536       psp_level6_pooling[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "psp_level1_bn (BatchNormalizati (None, 1, 1, 512)    2048        psp_level1_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "psp_level2_bn (BatchNormalizati (None, 2, 2, 512)    2048        psp_level2_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "psp_level3_bn (BatchNormalizati (None, 3, 3, 512)    2048        psp_level3_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "psp_level6_bn (BatchNormalizati (None, 6, 6, 512)    2048        psp_level6_conv[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "psp_level1_relu (Activation)    (None, 1, 1, 512)    0           psp_level1_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "psp_level2_relu (Activation)    (None, 2, 2, 512)    0           psp_level2_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "psp_level3_relu (Activation)    (None, 3, 3, 512)    0           psp_level3_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "psp_level6_relu (Activation)    (None, 6, 6, 512)    0           psp_level6_bn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "psp_level1_upsampling (UpSampli (None, 24, 24, 512)  0           psp_level1_relu[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "psp_level2_upsampling (UpSampli (None, 24, 24, 512)  0           psp_level2_relu[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "psp_level3_upsampling (UpSampli (None, 24, 24, 512)  0           psp_level3_relu[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "psp_level6_upsampling (UpSampli (None, 24, 24, 512)  0           psp_level6_relu[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "psp_concat (Concatenate)        (None, 24, 24, 2176) 0           stage3_unit1_relu1[0][0]         \n",
            "                                                                 psp_level1_upsampling[0][0]      \n",
            "                                                                 psp_level2_upsampling[0][0]      \n",
            "                                                                 psp_level3_upsampling[0][0]      \n",
            "                                                                 psp_level6_upsampling[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "aggregation_conv (Conv2D)       (None, 24, 24, 512)  1114112     psp_concat[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "aggregation_bn (BatchNormalizat (None, 24, 24, 512)  2048        aggregation_conv[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "aggregation_relu (Activation)   (None, 24, 24, 512)  0           aggregation_bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "final_conv (Conv2D)             (None, 24, 24, 8)    36872       aggregation_relu[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "final_upsampling (UpSampling2D) (None, 192, 192, 8)  0           final_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 192, 192, 8)  0           final_upsampling[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 2,778,321\n",
            "Trainable params: 2,770,123\n",
            "Non-trainable params: 8,198\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YL4QZdAbuoZ"
      },
      "source": [
        "# import imgaug.augmenters as iaa\n",
        "# For the assignment choose any 4 augumentation techniques\n",
        "# check the imgaug documentations for more augmentations\n",
        "aug2 = iaa.Fliplr(1)\n",
        "aug3 = iaa.Flipud(1)\n",
        "aug4 = iaa.Emboss(alpha=(1), strength=1)\n",
        "aug5 = iaa.DirectedEdgeDetect(alpha=(0.8), direction=(1.0))\n",
        "aug6 = iaa.Sharpen(alpha=(1.0), lightness=(1.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nweraVpDbuoa"
      },
      "source": [
        "def visualize(**images):\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(16, 5))\n",
        "    for i, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.title(' '.join(name.split('_')).title())\n",
        "        if i==1:\n",
        "            plt.imshow(image, cmap='gray', vmax=1, vmin=0)\n",
        "        else:\n",
        "            plt.imshow(image)\n",
        "    plt.show()\n",
        "    \n",
        "def normalize_image(img):\n",
        "    img = img/255\n",
        "    return img\n",
        "\n",
        "class Dataset:\n",
        "    \n",
        "    CLASSES = [0,1,2,3,4,5,6,7]\n",
        "    \n",
        "    def __init__(self, directory, img_files,  classes):\n",
        "        \n",
        "        self.img_ids = img_files\n",
        "        self.images_fps   = [os.path.join(directory, image_id.split('_')[0]+'_image.jpg') for image_id in self.img_ids]\n",
        "        self.masks_fps    = [os.path.join(directory, image_id.split('_')[0]+'_label.png') for image_id in self.img_ids]\n",
        "        self.class_values = [0,1,2,3,4,5,6,7]\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = cv2.imread(self.images_fps[i], cv2.IMREAD_UNCHANGED)\n",
        "        mask  = cv2.imread(self.masks_fps[i], cv2.IMREAD_UNCHANGED)\n",
        "        image = np.float32(cv2.resize(image,(192,192)))\n",
        "        mask = np.float32(cv2.resize(mask,(192,192)))\n",
        "        mask[mask==255] = 7\n",
        "        image = normalize_image(image)\n",
        "\n",
        "        image_masks = [(mask == v) for v in self.class_values]\n",
        "        image_mask = np.stack(image_masks, axis=-1).astype('float')\n",
        "\n",
        "        a = np.random.uniform()\n",
        "        if a<0.2:\n",
        "            image = aug2.augment_image(image)\n",
        "            image_mask = aug2.augment_image(image_mask)\n",
        "        elif a<0.4:\n",
        "            image = aug3.augment_image(image)\n",
        "            image_mask = aug3.augment_image(image_mask)\n",
        "        elif a<0.6:\n",
        "            image = aug4.augment_image(image)\n",
        "            image_mask = aug4.augment_image(image_mask)\n",
        "        elif a<0.8:\n",
        "            image = aug5.augment_image(image)\n",
        "            image_mask = image_mask\n",
        "        else:\n",
        "            image = aug6.augment_image(image)\n",
        "            image_mask = aug6.augment_image(image_mask)\n",
        "\n",
        "        return image, image_mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "    \n",
        "    \n",
        "class Dataloder(tf.keras.utils.Sequence):    \n",
        "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(dataset))\n",
        "\n",
        "    def __getitem__(self, i):       \n",
        "        # collect batch data\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "        \n",
        "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
        "        \n",
        "        return tuple(batch)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.indexes) // self.batch_size\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.indexes = np.random.permutation(self.indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT3hj1dGbuod"
      },
      "source": [
        "# https://github.com/qubvel/segmentation_models\n",
        "import segmentation_models as sm\n",
        "from segmentation_models.metrics import iou_score\n",
        "from segmentation_models import Unet\n",
        "\n",
        "optim = tf.keras.optimizers.Adam(0.0001)\n",
        "\n",
        "focal_loss = sm.losses.cce_dice_loss\n",
        "\n",
        "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
        "# total_loss = sm.losses.binary_focal_dice_loss \n",
        "# or total_loss = sm.losses.categorical_focal_dice_loss \n",
        "\n",
        "model.compile(optim, focal_loss, metrics=['accuracy',iou_score])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6MdKt4Tbuof"
      },
      "source": [
        "# Dataset for train images\n",
        "CLASSES = ['Drivable', 'Non_Drivable', 'Living_things', 'Vehicles', 'Road_side_objects', 'Far_objects', 'Sky', 'Other']\n",
        "    \n",
        "train_dataset = Dataset(train_path, X_train, classes=CLASSES)\n",
        "test_dataset  = Dataset(val_path, X_test, classes=CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "HDa3vPNMnoZe",
        "outputId": "68dd94f5-b913-4693-8e7e-99bd1ffc8b97"
      },
      "source": [
        "train_dataloader = Dataloder(train_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "print(train_dataloader[0][0].shape)\n",
        "assert train_dataloader[0][0].shape == (16, 192, 192, 3)\n",
        "assert train_dataloader[0][1].shape == (16, 192, 192, 8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16, 192, 192, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eigCK3TZbuoh",
        "outputId": "b2ff08b5-7ad2-4314-d786-651343f0064e"
      },
      "source": [
        "history = model.fit_generator(train_dataloader, steps_per_epoch=len(train_dataloader), epochs=50,\\\n",
        "                              validation_data=test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 56s 324ms/step - loss: 0.6927 - accuracy: 0.7007 - iou_score: 0.3307 - val_loss: 1.1258 - val_accuracy: 0.3242 - val_iou_score: 0.0811\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.5616 - accuracy: 0.7755 - iou_score: 0.4275 - val_loss: 1.3182 - val_accuracy: 0.3515 - val_iou_score: 0.0993\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.5139 - accuracy: 0.7985 - iou_score: 0.4656 - val_loss: 1.1319 - val_accuracy: 0.4567 - val_iou_score: 0.1664\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.4784 - accuracy: 0.8138 - iou_score: 0.4964 - val_loss: 0.7648 - val_accuracy: 0.6318 - val_iou_score: 0.3004\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.4549 - accuracy: 0.8233 - iou_score: 0.5180 - val_loss: 0.5992 - val_accuracy: 0.7234 - val_iou_score: 0.4027\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 50s 291ms/step - loss: 0.4376 - accuracy: 0.8302 - iou_score: 0.5330 - val_loss: 0.5339 - val_accuracy: 0.7803 - val_iou_score: 0.4730\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.4232 - accuracy: 0.8367 - iou_score: 0.5452 - val_loss: 0.4503 - val_accuracy: 0.8015 - val_iou_score: 0.5155\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.4107 - accuracy: 0.8418 - iou_score: 0.5560 - val_loss: 0.5303 - val_accuracy: 0.8045 - val_iou_score: 0.5060\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 50s 291ms/step - loss: 0.4036 - accuracy: 0.8455 - iou_score: 0.5607 - val_loss: 0.4561 - val_accuracy: 0.8082 - val_iou_score: 0.5014\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 51s 294ms/step - loss: 0.3904 - accuracy: 0.8511 - iou_score: 0.5751 - val_loss: 0.4671 - val_accuracy: 0.8145 - val_iou_score: 0.5216\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3815 - accuracy: 0.8547 - iou_score: 0.5818 - val_loss: 0.4644 - val_accuracy: 0.8139 - val_iou_score: 0.5202\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 50s 291ms/step - loss: 0.3754 - accuracy: 0.8571 - iou_score: 0.5869 - val_loss: 0.4706 - val_accuracy: 0.8122 - val_iou_score: 0.5172\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.3643 - accuracy: 0.8608 - iou_score: 0.5981 - val_loss: 0.4457 - val_accuracy: 0.8143 - val_iou_score: 0.5284\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3617 - accuracy: 0.8629 - iou_score: 0.5989 - val_loss: 0.4543 - val_accuracy: 0.8169 - val_iou_score: 0.5220\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3527 - accuracy: 0.8658 - iou_score: 0.6086 - val_loss: 0.4512 - val_accuracy: 0.8196 - val_iou_score: 0.5188\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3498 - accuracy: 0.8677 - iou_score: 0.6110 - val_loss: 0.4573 - val_accuracy: 0.8206 - val_iou_score: 0.5450\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 50s 291ms/step - loss: 0.3412 - accuracy: 0.8694 - iou_score: 0.6193 - val_loss: 0.4476 - val_accuracy: 0.8224 - val_iou_score: 0.5422\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 50s 291ms/step - loss: 0.3349 - accuracy: 0.8712 - iou_score: 0.6264 - val_loss: 0.4522 - val_accuracy: 0.8224 - val_iou_score: 0.5361\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 50s 291ms/step - loss: 0.3323 - accuracy: 0.8733 - iou_score: 0.6290 - val_loss: 0.4734 - val_accuracy: 0.8152 - val_iou_score: 0.5323\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.3236 - accuracy: 0.8751 - iou_score: 0.6376 - val_loss: 0.4593 - val_accuracy: 0.8249 - val_iou_score: 0.5447\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3251 - accuracy: 0.8760 - iou_score: 0.6346 - val_loss: 0.4678 - val_accuracy: 0.8212 - val_iou_score: 0.5348\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.3221 - accuracy: 0.8773 - iou_score: 0.6395 - val_loss: 0.4400 - val_accuracy: 0.8149 - val_iou_score: 0.5416\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 50s 291ms/step - loss: 0.3129 - accuracy: 0.8792 - iou_score: 0.6484 - val_loss: 0.4350 - val_accuracy: 0.8174 - val_iou_score: 0.5375\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3122 - accuracy: 0.8796 - iou_score: 0.6454 - val_loss: 0.4207 - val_accuracy: 0.8216 - val_iou_score: 0.5470\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3090 - accuracy: 0.8814 - iou_score: 0.6510 - val_loss: 0.4567 - val_accuracy: 0.8223 - val_iou_score: 0.5567\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3046 - accuracy: 0.8829 - iou_score: 0.6578 - val_loss: 0.4549 - val_accuracy: 0.8251 - val_iou_score: 0.5421\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 50s 291ms/step - loss: 0.3046 - accuracy: 0.8834 - iou_score: 0.6570 - val_loss: 0.4248 - val_accuracy: 0.8230 - val_iou_score: 0.5479\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3016 - accuracy: 0.8839 - iou_score: 0.6609 - val_loss: 0.4350 - val_accuracy: 0.8263 - val_iou_score: 0.5488\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.3009 - accuracy: 0.8849 - iou_score: 0.6584 - val_loss: 0.4531 - val_accuracy: 0.8257 - val_iou_score: 0.5404\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2960 - accuracy: 0.8862 - iou_score: 0.6617 - val_loss: 0.4188 - val_accuracy: 0.8267 - val_iou_score: 0.5551\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 51s 295ms/step - loss: 0.2913 - accuracy: 0.8875 - iou_score: 0.6712 - val_loss: 0.4784 - val_accuracy: 0.8276 - val_iou_score: 0.5550\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2921 - accuracy: 0.8878 - iou_score: 0.6695 - val_loss: 0.4336 - val_accuracy: 0.8217 - val_iou_score: 0.5507\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2912 - accuracy: 0.8884 - iou_score: 0.6684 - val_loss: 0.4310 - val_accuracy: 0.8230 - val_iou_score: 0.5424\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2892 - accuracy: 0.8896 - iou_score: 0.6698 - val_loss: 0.4308 - val_accuracy: 0.8281 - val_iou_score: 0.5395\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2854 - accuracy: 0.8906 - iou_score: 0.6775 - val_loss: 0.4043 - val_accuracy: 0.8282 - val_iou_score: 0.5485\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2787 - accuracy: 0.8917 - iou_score: 0.6833 - val_loss: 0.4173 - val_accuracy: 0.8248 - val_iou_score: 0.5517\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2778 - accuracy: 0.8917 - iou_score: 0.6827 - val_loss: 0.4586 - val_accuracy: 0.8269 - val_iou_score: 0.5575\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2798 - accuracy: 0.8919 - iou_score: 0.6796 - val_loss: 0.4548 - val_accuracy: 0.8290 - val_iou_score: 0.5509\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2761 - accuracy: 0.8934 - iou_score: 0.6865 - val_loss: 0.4126 - val_accuracy: 0.8190 - val_iou_score: 0.5570\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.2744 - accuracy: 0.8938 - iou_score: 0.6856 - val_loss: 0.4333 - val_accuracy: 0.8257 - val_iou_score: 0.5590\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 51s 294ms/step - loss: 0.2707 - accuracy: 0.8945 - iou_score: 0.6907 - val_loss: 0.4037 - val_accuracy: 0.8269 - val_iou_score: 0.5571\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 51s 295ms/step - loss: 0.2692 - accuracy: 0.8957 - iou_score: 0.6932 - val_loss: 0.4789 - val_accuracy: 0.8272 - val_iou_score: 0.5535\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2648 - accuracy: 0.8962 - iou_score: 0.6975 - val_loss: 0.4413 - val_accuracy: 0.8289 - val_iou_score: 0.5528\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 51s 294ms/step - loss: 0.2624 - accuracy: 0.8962 - iou_score: 0.7001 - val_loss: 0.4471 - val_accuracy: 0.8298 - val_iou_score: 0.5724\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2676 - accuracy: 0.8967 - iou_score: 0.6934 - val_loss: 0.4827 - val_accuracy: 0.8272 - val_iou_score: 0.5456\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2599 - accuracy: 0.8968 - iou_score: 0.7004 - val_loss: 0.3968 - val_accuracy: 0.8191 - val_iou_score: 0.5539\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.2598 - accuracy: 0.8975 - iou_score: 0.7035 - val_loss: 0.4671 - val_accuracy: 0.8274 - val_iou_score: 0.5593\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 50s 293ms/step - loss: 0.2592 - accuracy: 0.8981 - iou_score: 0.7028 - val_loss: 0.4484 - val_accuracy: 0.8252 - val_iou_score: 0.5527\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.2558 - accuracy: 0.8989 - iou_score: 0.7055 - val_loss: 0.4223 - val_accuracy: 0.8262 - val_iou_score: 0.5638\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 50s 292ms/step - loss: 0.2573 - accuracy: 0.8991 - iou_score: 0.7042 - val_loss: 0.4005 - val_accuracy: 0.8308 - val_iou_score: 0.5499\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "OxForupXbuoi",
        "outputId": "f4d9e6ac-cb17-426c-e2ab-d124ee45fe42"
      },
      "source": [
        "# Plot training & validation iou_score values\n",
        "plt.figure(figsize=(30, 5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history['iou_score'])\n",
        "plt.plot(history.history['val_iou_score'])\n",
        "plt.title('Model iou_score')\n",
        "plt.ylabel('iou_score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIK8bq3Nb9MY"
      },
      "source": [
        "HEIGHT = 192\n",
        "WIDTH = 192\n",
        "\n",
        "def parse_image(img_path):\n",
        "    \"\"\"\n",
        "    Load an image and its annotation (mask) and returning a dictionary.\n",
        "    \"\"\"\n",
        "    # Reading the image\n",
        "    image = tf.io.read_file(img_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    # For one Image path:\n",
        "    # .../idd20k_lite/leftImg8bit/train/024541_image.jpg\n",
        "    # Its corresponding annotation path is:\n",
        "    # .../idd20k_lite/gtFine/train/024541_label.png\n",
        "    mask_path = tf.strings.regex_replace(img_path, \"leftImg8bit\", \"gtFine\")\n",
        "    mask_path = tf.strings.regex_replace(mask_path, \"_image.jpg\", \"_label.png\")\n",
        "    # Reading the annotation file corresponding the image file\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    # In scene parsing, \"not labeled\" = 255\n",
        "    # But it will mess with our N_CLASS = 7\n",
        "    # Since 255 means the 255th class\n",
        "    # Which doesn't exist\n",
        "    mask = tf.where(mask==255, np.dtype('uint8').type(7), mask)\n",
        "    # Note that we have to convert the new value (7)\n",
        "    # With the same dtype than the tensor itself\n",
        "    return {'image': image, 'segmentation_mask': mask}\n",
        "\n",
        "# Reference -> https://www.tensorflow.org/api_docs/python/tf/cast\n",
        "# Returns a Tensor same shape as given tensor and same type as dtype that is mentioned.\n",
        "\n",
        "def normalize(input_image, input_mask):\n",
        "    \"\"\"\n",
        "    Rescale the pixel values of the images between 0 and 1 compared to [0,255] originally.\n",
        "    \"\"\"\n",
        "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "    return input_image, input_mask\n",
        "\n",
        "def load_image_test(datapoint):\n",
        "    \"\"\"\n",
        "    Normalize and resize a test image and its annotation.\n",
        "    Since this is for the test set, we don't need to apply any data augmentation technique.\n",
        "    \"\"\"\n",
        "    input_image = tf.image.resize(datapoint['image'], (HEIGHT,WIDTH))\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT,WIDTH))\n",
        "    input_image, input_mask = normalize(input_image, input_mask)\n",
        "    return input_image, input_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcEcEIvMJDJO"
      },
      "source": [
        "# Reference -> https://github.com/saisandeepNSS/IDD_SemanticSegmentation\n",
        "\n",
        "def IoU(y_i,y_pred):\n",
        "    # This function calculates the mean Intersection over Union\n",
        "    # Mean IoU = TP/(FN + TP + FP)\n",
        "    # This list will save the IoU of all the classes\n",
        "    IoUs = []\n",
        "    # Defining the number of classes which the model has predicted\n",
        "    n_classes = 8\n",
        "    for c in range(n_classes):\n",
        "        # Calculating the True Positives\n",
        "        TP = np.sum((y_i == c)&(y_pred==c))\n",
        "        # Calculating the False Positives\n",
        "        FP = np.sum((y_i != c)&(y_pred==c))\n",
        "        # Calculating the False Negatives\n",
        "        FN = np.sum((y_i == c)&(y_pred!= c))\n",
        "        # Calculating the IoU for the particular class\n",
        "        IoU = TP/float(TP + FP + FN)\n",
        "        # Printing the outputs\n",
        "        # Uncomment the print statement below when you want to analyze the results for each class\n",
        "        #print(\"class {:02.0f}: #TP={:6.0f}, #FP={:6.0f}, #FN={:5.0f}, IoU={:4.3f}\".format(c,TP,FP,FN,IoU))\n",
        "        # Appending the IoU to the list as it mean needs to be calculated later\n",
        "        if(math.isnan(IoU)):\n",
        "            IoUs.append(0)\n",
        "            continue\n",
        "        IoUs.append(IoU)\n",
        "    # Calculating the mean\n",
        "    mIoU = np.mean(IoUs)\n",
        "    #print(\"_________________\")\n",
        "    #print(\"Mean IoU: {:4.3f}\".format(mIoU))\n",
        "    return mIoU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epCBdc3oJuGU"
      },
      "source": [
        "def predict(model,image_path):\n",
        "    \"\"\"\n",
        "    This function will take the model which is going to be used to predict the image and the image path of \n",
        "    the input image as inputs and predict the mask\n",
        "    It returns the true mask and predicted mask\n",
        "    \"\"\"\n",
        "    # Getting the datapoint\n",
        "    # This function will load the image and its annotation (mask) and return a dictionary.\n",
        "    datapoint = parse_image(image_path)\n",
        "    # Normalizing the resizing the datapoint\n",
        "    input_image,image_mask = load_image_test(datapoint)\n",
        "    # As the model takes input with 4 dimensions (batch_size, rows, columns, channels),\n",
        "    # and the shape of the input image is (rows, columns, channels)\n",
        "    # we will expand the first dimension so we will get the shape as  (1, rows, columns, channels)\n",
        "    img = tf.expand_dims(input_image, 0)\n",
        "    # Predicting the image by passing it to the model\n",
        "    prediction = model.predict_on_batch(img)\n",
        "    # The model will predict 8 outputs for each pixel\n",
        "    # We have to get the maximum value out of it\n",
        "    prediction = tf.argmax(prediction, axis=-1)\n",
        "    prediction = tf.squeeze(prediction, axis = 0)\n",
        "    pred_mask = tf.expand_dims(prediction, axis=-1)\n",
        "    # Displaying the input image, true mask, predicted mask\n",
        "    #display_sample([input_image, image_mask, pred_mask])\n",
        "    return image_mask, pred_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i04dwyBh_4Ib"
      },
      "source": [
        "### Validation mIoU for PSPNet imported model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O9we200JP3D"
      },
      "source": [
        "img_val = dataset_path + 'leftImg8bit/val/'\n",
        "val_paths = glob(img_val+'*/*_image.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "eTz1GK53JJBF",
        "outputId": "cf9aed40-a4c1-4bfb-8e61-687621825050"
      },
      "source": [
        "mIoU = []\n",
        "for path in val_paths:\n",
        "    true_mask, pred_mask = predict(model,path)\n",
        "    mIoU.append(IoU(true_mask, pred_mask))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "V7ZAZG1Wdc-K",
        "outputId": "9651a8a8-e2da-41c4-fa6b-34668b0bc2fd"
      },
      "source": [
        "print(\"Validation mIoU Score : \",sum(mIoU)/len(mIoU))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation mIoU Score :  0.363701048404907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7YK3U_U_4Ic"
      },
      "source": [
        "# Observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEAB1NtU_4Ic",
        "outputId": "a10dc3f5-1d62-4cc9-bb64-c00046de539c"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Representing results in a table\n",
        "pt = PrettyTable()\n",
        "\n",
        "pt.field_names = [\"Model\", \"Highest Train Accuracy\", \"Highest Validation Accuracy\", \"Validation mIoU\"]\n",
        "pt.add_row([\"PSPNet (from Scratch)\",'89.76 %','80.80 %',0.43338])\n",
        "pt.add_row([\"PSPNet with a Pretrained Encoder\",'69.438 %','66.919 %',0.24952])\n",
        "pt.add_row([\"PSPNet (from official models)\",'89.91 %','83.08 %', 0.36370])\n",
        "print(pt)            "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------+------------------------+-----------------------------+-----------------+\n",
            "|              Model               | Highest Train Accuracy | Highest Validation Accuracy | Validation mIoU |\n",
            "+----------------------------------+------------------------+-----------------------------+-----------------+\n",
            "|      PSPNet (from Scratch)       |        89.76 %         |           80.80 %           |     0.43338     |\n",
            "| PSPNet with a Pretrained Encoder |        69.438 %        |           66.919 %          |     0.24952     |\n",
            "|  PSPNet (from official models)   |        89.91 %         |           83.08 %           |      0.3637     |\n",
            "+----------------------------------+------------------------+-----------------------------+-----------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnucsZCP_4Ic"
      },
      "source": [
        "* The best results are found using PSPNet from Scratch model.\n",
        "* When we train PSPNet using a pretrained encoder, the results fluctuate a lot and the validation mIoU is very low as well.\n",
        "* While using the official model, we get a high validation accuracy, but the validation mIoU score is low."
      ]
    }
  ]
}