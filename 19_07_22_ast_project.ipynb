{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19_07_22_ast_project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNStw1ENgcEQ6aTgKWfmdIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TIMEdilation584/JP_Loksatta_moving_hearts/blob/master/19_07_22_ast_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "0Tyz39bWHGHX",
        "outputId": "3bc25bff-1fc8-444b-f54c-2123f7950772"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-03e756c67dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mASTModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# download pretrained model in this directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TORCH_HOME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../pretrained_models'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# @Time    : 6/24/21 12:50 AM\n",
        "# @Author  : Yuan Gong\n",
        "# @Affiliation  : Massachusetts Institute of Technology\n",
        "# @Email   : yuangong@mit.edu\n",
        "# @File    : demo.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from models import ASTModel\n",
        "# download pretrained model in this directory\n",
        "os.environ['TORCH_HOME'] = '../pretrained_models'\n",
        "# assume each input spectrogram has 100 time frames\n",
        "input_tdim = 100\n",
        "# assume the task has 527 classes\n",
        "label_dim = 527\n",
        "# create a pseudo input: a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n",
        "test_input = torch.rand([10, input_tdim, 128])\n",
        "# create an AST model\n",
        "ast_mdl = ASTModel(label_dim=label_dim, input_tdim=input_tdim, imagenet_pretrain=True, audioset_pretrain=True)\n",
        "test_output = ast_mdl(test_input)\n",
        "# output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n",
        "print(test_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/YuanGongND/ast.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVMZ2IhoHiMS",
        "outputId": "916ca1ab-8f90-498c-eba4-c4788c8f87cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ast' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ast/ "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3ep0UiEHtXq",
        "outputId": "6d2ff03c-45fe-4878-b48c-fa09245a1d2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv"
      ],
      "metadata": {
        "id": "cvLw7-ZvHxAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!virtualenv venvast"
      ],
      "metadata": {
        "id": "rpVgO3hjH2OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source venvast/bin/activate"
      ],
      "metadata": {
        "id": "4uLsDAZeH69J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "xgNHvaK3H6_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "# @Time    : 6/19/21 4:31 PM\n",
        "# @Author  : Yuan Gong\n",
        "# @Affiliation  : Massachusetts Institute of Technology\n",
        "# @Email   : yuangong@mit.edu\n",
        "# @File    : __init__.py\n",
        "\n"
      ],
      "metadata": {
        "id": "ey3CRk7sHTfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from .ast_models import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "Hxr3p5RiJ5cw",
        "outputId": "3d374996-3bbd-47bc-a444-c9812c3fa03d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-360c49cd0f20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mast_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# @Time    : 6/10/21 5:04 PM\n",
        "# @Author  : Yuan Gong\n",
        "# @Affiliation  : Massachusetts Institute of Technology\n",
        "# @Email   : yuangong@mit.edu\n",
        "# @File    : ast_models.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast\n",
        "import os\n",
        "import wget\n",
        "os.environ['TORCH_HOME'] = '../../pretrained_models'\n",
        "import timm\n",
        "from timm.models.layers import to_2tuple,trunc_normal_\n",
        "\n",
        "# override the timm package to relax the input shape constraint.\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class ASTModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The AST model.\n",
        "    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
        "    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
        "    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
        "    :param input_fdim: the number of frequency bins of the input spectrogram\n",
        "    :param input_tdim: the number of time frames of the input spectrogram\n",
        "    :param imagenet_pretrain: if use ImageNet pretrained model\n",
        "    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n",
        "    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
        "    \"\"\"\n",
        "    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True):\n",
        "\n",
        "        super(ASTModel, self).__init__()\n",
        "        assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n",
        "\n",
        "        if verbose == True:\n",
        "            print('---------------AST Model Summary---------------')\n",
        "            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n",
        "        # override timm input shape restriction\n",
        "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
        "\n",
        "        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n",
        "        if audioset_pretrain == False:\n",
        "            if model_size == 'tiny224':\n",
        "                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
        "            elif model_size == 'small224':\n",
        "                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
        "            elif model_size == 'base224':\n",
        "                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
        "            elif model_size == 'base384':\n",
        "                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n",
        "            else:\n",
        "                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n",
        "            self.original_num_patches = self.v.patch_embed.num_patches\n",
        "            self.oringal_hw = int(self.original_num_patches ** 0.5)\n",
        "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
        "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
        "\n",
        "            # automatcially get the intermediate shape\n",
        "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
        "            num_patches = f_dim * t_dim\n",
        "            self.v.patch_embed.num_patches = num_patches\n",
        "            if verbose == True:\n",
        "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
        "                print('number of patches={:d}'.format(num_patches))\n",
        "\n",
        "            # the linear projection layer\n",
        "            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
        "            if imagenet_pretrain == True:\n",
        "                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n",
        "                new_proj.bias = self.v.patch_embed.proj.bias\n",
        "            self.v.patch_embed.proj = new_proj\n",
        "\n",
        "            # the positional embedding\n",
        "            if imagenet_pretrain == True:\n",
        "                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n",
        "                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n",
        "                # cut (from middle) or interpolate the second dimension of the positional embedding\n",
        "                if t_dim <= self.oringal_hw:\n",
        "                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n",
        "                else:\n",
        "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n",
        "                # cut (from middle) or interpolate the first dimension of the positional embedding\n",
        "                if f_dim <= self.oringal_hw:\n",
        "                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n",
        "                else:\n",
        "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
        "                # flatten the positional embedding\n",
        "                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n",
        "                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n",
        "                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
        "            else:\n",
        "                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n",
        "                # TODO can use sinusoidal positional embedding instead\n",
        "                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n",
        "                self.v.pos_embed = new_pos_embed\n",
        "                trunc_normal_(self.v.pos_embed, std=.02)\n",
        "\n",
        "        # now load a model that is pretrained on both ImageNet and AudioSet\n",
        "        elif audioset_pretrain == True:\n",
        "            if audioset_pretrain == True and imagenet_pretrain == False:\n",
        "                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n",
        "            if model_size != 'base384':\n",
        "                raise ValueError('currently only has base384 AudioSet pretrained model.')\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            if os.path.exists('../../pretrained_models/audioset_10_10_0.4593.pth') == False:\n",
        "                # this model performs 0.4593 mAP on the audioset eval set\n",
        "                audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
        "                wget.download(audioset_mdl_url, out='../../pretrained_models/audioset_10_10_0.4593.pth')\n",
        "            sd = torch.load('../../pretrained_models/audioset_10_10_0.4593.pth', map_location=device)\n",
        "            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n",
        "            audio_model = torch.nn.DataParallel(audio_model)\n",
        "            audio_model.load_state_dict(sd, strict=False)\n",
        "            self.v = audio_model.module.v\n",
        "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
        "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
        "\n",
        "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
        "            num_patches = f_dim * t_dim\n",
        "            self.v.patch_embed.num_patches = num_patches\n",
        "            if verbose == True:\n",
        "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
        "                print('number of patches={:d}'.format(num_patches))\n",
        "\n",
        "            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n",
        "            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n",
        "            if t_dim < 101:\n",
        "                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n",
        "            # otherwise interpolate\n",
        "            else:\n",
        "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n",
        "            if f_dim < 12:\n",
        "                new_pos_embed = new_pos_embed[:, :, 6 - int(f_dim/2): 6 - int(f_dim/2) + f_dim, :]\n",
        "            # otherwise interpolate\n",
        "            elif f_dim > 12:\n",
        "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
        "            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n",
        "            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
        "\n",
        "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n",
        "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
        "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
        "        test_out = test_proj(test_input)\n",
        "        f_dim = test_out.shape[2]\n",
        "        t_dim = test_out.shape[3]\n",
        "        return f_dim, t_dim\n",
        "\n",
        "    @autocast()\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
        "        :return: prediction\n",
        "        \"\"\"\n",
        "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = x.transpose(2, 3)\n",
        "\n",
        "        B = x.shape[0]\n",
        "        x = self.v.patch_embed(x)\n",
        "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
        "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
        "        x = x + self.v.pos_embed\n",
        "        x = self.v.pos_drop(x)\n",
        "        for blk in self.v.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.v.norm(x)\n",
        "        x = (x[:, 0] + x[:, 1]) / 2\n",
        "\n",
        "        x = self.mlp_head(x)\n",
        "        return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_tdim = 100\n",
        "    ast_mdl = ASTModel(input_tdim=input_tdim)\n",
        "    # input a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n",
        "    test_input = torch.rand([10, input_tdim, 128])\n",
        "    test_output = ast_mdl(test_input)\n",
        "    # output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n",
        "    print(test_output.shape)\n",
        "\n",
        "    input_tdim = 256\n",
        "    ast_mdl = ASTModel(input_tdim=input_tdim,label_dim=50, audioset_pretrain=True)\n",
        "    # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins\n",
        "    test_input = torch.rand([10, input_tdim, 128])\n",
        "    test_output = ast_mdl(test_input)\n",
        "    # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.\n",
        "    print(test_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEc0hhrGHZnx",
        "outputId": "c70a7984-9e10-497e-abe3-e6ef03f7cd47"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: True, AudioSet pretraining: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth\" to ../../pretrained_models/hub/checkpoints/deit_base_distilled_patch16_384-d0272ac0.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frequncey stride=10, time stride=10\n",
            "number of patches=108\n",
            "torch.Size([10, 527])\n",
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: True, AudioSet pretraining: True\n",
            "frequncey stride=10, time stride=10\n",
            "number of patches=300\n",
            "torch.Size([10, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast\n",
        "import os\n",
        "import wget\n",
        "os.environ['TORCH_HOME'] = '../../pretrained_models'\n",
        "import timm\n",
        "from timm.models.layers import to_2tuple,trunc_normal_\n",
        "\n",
        "# override the timm package to relax the input shape constraint.\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class ASTModel(nn.Module):\n",
        "    \"\"\"\n",
        "    The AST model.\n",
        "    :param label_dim: the label dimension, i.e., the number of total classes, it is 527 for AudioSet, 50 for ESC-50, and 35 for speechcommands v2-35\n",
        "    :param fstride: the stride of patch spliting on the frequency dimension, for 16*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6\n",
        "    :param tstride: the stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6\n",
        "    :param input_fdim: the number of frequency bins of the input spectrogram\n",
        "    :param input_tdim: the number of time frames of the input spectrogram\n",
        "    :param imagenet_pretrain: if use ImageNet pretrained model\n",
        "    :param audioset_pretrain: if use full AudioSet and ImageNet pretrained model\n",
        "    :param model_size: the model size of AST, should be in [tiny224, small224, base224, base384], base224 and base 384 are same model, but are trained differently during ImageNet pretraining.\n",
        "    \"\"\"\n",
        "    def __init__(self, label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=True, audioset_pretrain=False, model_size='base384', verbose=True):\n",
        "\n",
        "        super(ASTModel, self).__init__()\n",
        "        assert timm.__version__ == '0.4.5', 'Please use timm == 0.4.5, the code might not be compatible with newer versions.'\n",
        "\n",
        "        if verbose == True:\n",
        "            print('---------------AST Model Summary---------------')\n",
        "            print('ImageNet pretraining: {:s}, AudioSet pretraining: {:s}'.format(str(imagenet_pretrain),str(audioset_pretrain)))\n",
        "        # override timm input shape restriction\n",
        "        timm.models.vision_transformer.PatchEmbed = PatchEmbed\n",
        "\n",
        "        # if AudioSet pretraining is not used (but ImageNet pretraining may still apply)\n",
        "        if audioset_pretrain == False:\n",
        "            if model_size == 'tiny224':\n",
        "                self.v = timm.create_model('vit_deit_tiny_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
        "            elif model_size == 'small224':\n",
        "                self.v = timm.create_model('vit_deit_small_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
        "            elif model_size == 'base224':\n",
        "                self.v = timm.create_model('vit_deit_base_distilled_patch16_224', pretrained=imagenet_pretrain)\n",
        "            elif model_size == 'base384':\n",
        "                self.v = timm.create_model('vit_deit_base_distilled_patch16_384', pretrained=imagenet_pretrain)\n",
        "            else:\n",
        "                raise Exception('Model size must be one of tiny224, small224, base224, base384.')\n",
        "            self.original_num_patches = self.v.patch_embed.num_patches\n",
        "            self.oringal_hw = int(self.original_num_patches ** 0.5)\n",
        "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
        "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
        "\n",
        "            # automatcially get the intermediate shape\n",
        "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
        "            num_patches = f_dim * t_dim\n",
        "            self.v.patch_embed.num_patches = num_patches\n",
        "            if verbose == True:\n",
        "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
        "                print('number of patches={:d}'.format(num_patches))\n",
        "\n",
        "            # the linear projection layer\n",
        "            new_proj = torch.nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
        "            if imagenet_pretrain == True:\n",
        "                new_proj.weight = torch.nn.Parameter(torch.sum(self.v.patch_embed.proj.weight, dim=1).unsqueeze(1))\n",
        "                new_proj.bias = self.v.patch_embed.proj.bias\n",
        "            self.v.patch_embed.proj = new_proj\n",
        "\n",
        "            # the positional embedding\n",
        "            if imagenet_pretrain == True:\n",
        "                # get the positional embedding from deit model, skip the first two tokens (cls token and distillation token), reshape it to original 2D shape (24*24).\n",
        "                new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, self.original_num_patches, self.original_embedding_dim).transpose(1, 2).reshape(1, self.original_embedding_dim, self.oringal_hw, self.oringal_hw)\n",
        "                # cut (from middle) or interpolate the second dimension of the positional embedding\n",
        "                if t_dim <= self.oringal_hw:\n",
        "                    new_pos_embed = new_pos_embed[:, :, :, int(self.oringal_hw / 2) - int(t_dim / 2): int(self.oringal_hw / 2) - int(t_dim / 2) + t_dim]\n",
        "                else:\n",
        "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(self.oringal_hw, t_dim), mode='bilinear')\n",
        "                # cut (from middle) or interpolate the first dimension of the positional embedding\n",
        "                if f_dim <= self.oringal_hw:\n",
        "                    new_pos_embed = new_pos_embed[:, :, int(self.oringal_hw / 2) - int(f_dim / 2): int(self.oringal_hw / 2) - int(f_dim / 2) + f_dim, :]\n",
        "                else:\n",
        "                    new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
        "                # flatten the positional embedding\n",
        "                new_pos_embed = new_pos_embed.reshape(1, self.original_embedding_dim, num_patches).transpose(1,2)\n",
        "                # concatenate the above positional embedding with the cls token and distillation token of the deit model.\n",
        "                self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
        "            else:\n",
        "                # if not use imagenet pretrained model, just randomly initialize a learnable positional embedding\n",
        "                # TODO can use sinusoidal positional embedding instead\n",
        "                new_pos_embed = nn.Parameter(torch.zeros(1, self.v.patch_embed.num_patches + 2, self.original_embedding_dim))\n",
        "                self.v.pos_embed = new_pos_embed\n",
        "                trunc_normal_(self.v.pos_embed, std=.02)\n",
        "\n",
        "        # now load a model that is pretrained on both ImageNet and AudioSet\n",
        "        elif audioset_pretrain == True:\n",
        "            if audioset_pretrain == True and imagenet_pretrain == False:\n",
        "                raise ValueError('currently model pretrained on only audioset is not supported, please set imagenet_pretrain = True to use audioset pretrained model.')\n",
        "            if model_size != 'base384':\n",
        "                raise ValueError('currently only has base384 AudioSet pretrained model.')\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            if os.path.exists('../../pretrained_models/audioset_10_10_0.4593.pth') == False:\n",
        "                # this model performs 0.4593 mAP on the audioset eval set\n",
        "                audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
        "                wget.download(audioset_mdl_url, out='../../pretrained_models/audioset_10_10_0.4593.pth')\n",
        "            sd = torch.load('../../pretrained_models/audioset_10_10_0.4593.pth', map_location=device)\n",
        "            audio_model = ASTModel(label_dim=527, fstride=10, tstride=10, input_fdim=128, input_tdim=1024, imagenet_pretrain=False, audioset_pretrain=False, model_size='base384', verbose=False)\n",
        "            audio_model = torch.nn.DataParallel(audio_model)\n",
        "            audio_model.load_state_dict(sd, strict=False)\n",
        "            self.v = audio_model.module.v\n",
        "            self.original_embedding_dim = self.v.pos_embed.shape[2]\n",
        "            self.mlp_head = nn.Sequential(nn.LayerNorm(self.original_embedding_dim), nn.Linear(self.original_embedding_dim, label_dim))\n",
        "\n",
        "            f_dim, t_dim = self.get_shape(fstride, tstride, input_fdim, input_tdim)\n",
        "            num_patches = f_dim * t_dim\n",
        "            self.v.patch_embed.num_patches = num_patches\n",
        "            if verbose == True:\n",
        "                print('frequncey stride={:d}, time stride={:d}'.format(fstride, tstride))\n",
        "                print('number of patches={:d}'.format(num_patches))\n",
        "\n",
        "            new_pos_embed = self.v.pos_embed[:, 2:, :].detach().reshape(1, 1212, 768).transpose(1, 2).reshape(1, 768, 12, 101)\n",
        "            # if the input sequence length is larger than the original audioset (10s), then cut the positional embedding\n",
        "            if t_dim < 101:\n",
        "                new_pos_embed = new_pos_embed[:, :, :, 50 - int(t_dim/2): 50 - int(t_dim/2) + t_dim]\n",
        "            # otherwise interpolate\n",
        "            else:\n",
        "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(12, t_dim), mode='bilinear')\n",
        "            if f_dim < 12:\n",
        "                new_pos_embed = new_pos_embed[:, :, 6 - int(f_dim/2): 6 - int(f_dim/2) + f_dim, :]\n",
        "            # otherwise interpolate\n",
        "            elif f_dim > 12:\n",
        "                new_pos_embed = torch.nn.functional.interpolate(new_pos_embed, size=(f_dim, t_dim), mode='bilinear')\n",
        "            new_pos_embed = new_pos_embed.reshape(1, 768, num_patches).transpose(1, 2)\n",
        "            self.v.pos_embed = nn.Parameter(torch.cat([self.v.pos_embed[:, :2, :].detach(), new_pos_embed], dim=1))\n",
        "\n",
        "    def get_shape(self, fstride, tstride, input_fdim=128, input_tdim=1024):\n",
        "        test_input = torch.randn(1, 1, input_fdim, input_tdim)\n",
        "        test_proj = nn.Conv2d(1, self.original_embedding_dim, kernel_size=(16, 16), stride=(fstride, tstride))\n",
        "        test_out = test_proj(test_input)\n",
        "        f_dim = test_out.shape[2]\n",
        "        t_dim = test_out.shape[3]\n",
        "        return f_dim, t_dim\n",
        "\n",
        "    @autocast()\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: the input spectrogram, expected shape: (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
        "        :return: prediction\n",
        "        \"\"\"\n",
        "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = x.transpose(2, 3)\n",
        "\n",
        "        B = x.shape[0]\n",
        "        x = self.v.patch_embed(x)\n",
        "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
        "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
        "        x = x + self.v.pos_embed\n",
        "        x = self.v.pos_drop(x)\n",
        "        for blk in self.v.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.v.norm(x)\n",
        "        x = (x[:, 0] + x[:, 1]) / 2\n",
        "\n",
        "        x = self.mlp_head(x)\n",
        "        return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_tdim = 100\n",
        "    ast_mdl = ASTModel(input_tdim=input_tdim)\n",
        "    # input a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins\n",
        "    test_input = torch.rand([10, input_tdim, 128])\n",
        "    test_output = ast_mdl(test_input)\n",
        "    # output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.\n",
        "    print(test_output.shape)\n",
        "\n",
        "    input_tdim = 256\n",
        "    ast_mdl = ASTModel(input_tdim=input_tdim,label_dim=50, audioset_pretrain=True)\n",
        "    # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins\n",
        "    test_input = torch.rand([10, input_tdim, 128])\n",
        "    test_output = ast_mdl(test_input)\n",
        "    # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.\n",
        "    print(test_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcecuC92M6jj",
        "outputId": "cdcd2a51-22c8-4bfb-9f9f-95e209b1d2b1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/autocast_mode.py:118: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: True, AudioSet pretraining: False\n",
            "frequncey stride=10, time stride=10\n",
            "number of patches=108\n",
            "torch.Size([10, 527])\n",
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: True, AudioSet pretraining: True\n",
            "frequncey stride=10, time stride=10\n",
            "number of patches=300\n",
            "torch.Size([10, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ASTModel(label_dim=527, fshape=16, tshape=16 ,fstride=10, tstride=10, input_fdim=128, input_tdim=1024, model_size='base',\n",
        "         pretrain_stage=True, load_pretrained_mdl_path=None)"
      ],
      "metadata": {
        "id": "oBKQ3b0lMgOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fCR6Dim8QRPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_to_target(base_data_path = '../data/'):\n",
        "\n",
        "    # audio files and their corresponding labels\n",
        "    train_path = base_data_path + \"train/Train/*.wav\"\n",
        "    train_label_path = base_data_path +  \"train.csv\"\n",
        "    test_path =  base_data_path + \"test/Test/*.wav\"\n"
      ],
      "metadata": {
        "id": "_RB7icgQQPTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os \n",
        "import torch\n",
        "#from models import ASTModel \n",
        "# download pretrained model in this directory\n",
        "os.environ['TORCH_HOME'] = '../pretrained_models'  \n",
        "# assume each input spectrogram has 100 time frames\n",
        "input_tdim = 100\n",
        "# assume the task has 527 classes\n",
        "label_dim = 527\n",
        "# create a pseudo input: a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins \n",
        "test_input = torch.rand([10, input_tdim, 128]) \n",
        "# create an AST model\n",
        "ast_mdl = ASTModel(label_dim=label_dim, input_tdim=input_tdim, imagenet_pretrain=True)\n",
        "test_output = ast_mdl(test_input) \n",
        "# output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes. \n",
        "print(test_output.shape)  "
      ],
      "metadata": {
        "id": "TFFiS_-VHPr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n"
      ],
      "metadata": {
        "id": "ZBg1NBCnPH2B"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_output.cpu().detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "IuPOxw34N7QS",
        "outputId": "19cdaaa6-33a5-413e-e27d-f2c0c2b206d7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efdf3c56690>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "torchaudio.set_audio_backend(\"soundfile\")       # switch backend\n",
        "basepath = os.path.dirname(os.path.dirname(sys.path[0]))\n",
        "sys.path.append(basepath)\n",
        "#from src.models import ASTModel\n",
        "\n",
        "# download pretrained model in this directory\n",
        "os.environ['TORCH_HOME'] = '../pretrained_models'\n",
        "\n",
        "\n",
        "def make_features(wav_name, mel_bins, target_length=1024):\n",
        "    waveform, sr = torchaudio.load(wav_name)\n",
        "\n",
        "    fbank = torchaudio.compliance.kaldi.fbank(\n",
        "        waveform, htk_compat=True, sample_frequency=sr, use_energy=False,\n",
        "        window_type='hanning', num_mel_bins=mel_bins, dither=0.0,\n",
        "        frame_shift=10)\n",
        "\n",
        "    n_frames = fbank.shape[0]\n",
        "\n",
        "    p = target_length - n_frames\n",
        "    if p > 0:\n",
        "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
        "        fbank = m(fbank)\n",
        "    elif p < 0:\n",
        "        fbank = fbank[0:target_length, :]\n",
        "\n",
        "    fbank = (fbank - (-4.2677393)) / (4.5689974 * 2)\n",
        "    return fbank\n",
        "\n",
        "\n",
        "def load_label(label_csv):\n",
        "    with open(label_csv, 'r') as f:\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        lines = list(reader)\n",
        "    labels = []\n",
        "    ids = []  # Each label has a unique id such as \"/m/068hy\"\n",
        "    for i1 in range(1, len(lines)):\n",
        "        id = lines[i1][1]\n",
        "        label = lines[i1][2]\n",
        "        ids.append(id)\n",
        "        labels.append(label)\n",
        "    return labels\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Example of parser:'\n",
        "                                                 'python inference --audio_path ./0OxlgIitVig.wav '\n",
        "                                                 '--model_path ./pretrained_models/audioset_10_10_0.4593.pth')\n",
        "\n",
        "    parser.add_argument(\"--model_path\", type=str, required=True,\n",
        "                        help=\"the trained model you want to test\")\n",
        "    parser.add_argument('--audio_path',\n",
        "                        help='the audio you want to predict, sample rate 16k.',\n",
        "                        type=str, required=True)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    label_csv = './data/class_labels_indices.csv'       # label and indices for audioset data\n",
        "\n",
        "    # 1. make feature for predict\n",
        "    audio_path = args.audio_path\n",
        "    feats = make_features(audio_path, mel_bins=128)           # shape(1024, 128)\n",
        "\n",
        "    # assume each input spectrogram has 100 time frames\n",
        "    input_tdim = feats.shape[0]\n",
        "\n",
        "    # 2. load the best model and the weights\n",
        "    checkpoint_path = args.model_path\n",
        "    ast_mdl = ASTModel(label_dim=527, input_tdim=input_tdim, imagenet_pretrain=False, audioset_pretrain=False)\n",
        "    print(f'[*INFO] load checkpoint: {checkpoint_path}')\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "    audio_model = torch.nn.DataParallel(ast_mdl, device_ids=[0])\n",
        "    audio_model.load_state_dict(checkpoint)\n",
        "\n",
        "    audio_model = audio_model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "    # 3. feed the data feature to model\n",
        "    feats_data = feats.expand(1, input_tdim, 128)           # reshape the feature\n",
        "\n",
        "    audio_model.eval()                                      # set the eval model\n",
        "    with torch.no_grad():\n",
        "        output = audio_model.forward(feats_data)\n",
        "        output = torch.sigmoid(output)\n",
        "    result_output = output.data.cpu().numpy()[0]\n",
        "\n",
        "    # 4. map the post-prob to label\n",
        "    labels = load_label(label_csv)\n",
        "\n",
        "    sorted_indexes = np.argsort(result_output)[::-1]\n",
        "\n",
        "    # Print audio tagging top probabilities\n",
        "    print('[*INFO] predice results:')\n",
        "    for k in range(10):\n",
        "        print('{}: {:.4f}'.format(np.array(labels)[sorted_indexes[k]],\n",
        "                                  result_output[sorted_indexes[k]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "SnCrIKzcPGoJ",
        "outputId": "4c0a0b30-3610-4ec2-c1f7-06a7d6e3476c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] --model_path MODEL_PATH --audio_path\n",
            "                             AUDIO_PATH\n",
            "ipykernel_launcher.py: error: the following arguments are required: --model_path, --audio_path\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}